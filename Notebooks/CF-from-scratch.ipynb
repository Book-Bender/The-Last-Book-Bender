{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6431eef8-ed8d-4cf7-89f9-0917a29350bf",
   "metadata": {},
   "source": [
    "# Collaborative Filtering from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26415e0c-fda6-45d4-bfab-091ef1fb486b",
   "metadata": {},
   "source": [
    "This notebook is used to demonstrate how to code and evaluate CF (Collaborative Filtering) using `sklearn`, `pandas`, `scipy`.\n",
    "\n",
    "It is called \"from scratch\" as it does not use the `surprise` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a50f7b6-489f-4df7-9fac-6ab8f0f96614",
   "metadata": {},
   "source": [
    "## 1. Load libraries and data, inspect data, convert data as sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b49b870-f06e-46a5-97c0-fc40544d07ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327faa00-b97c-40b2-849b-4c8daa180a00",
   "metadata": {},
   "source": [
    "Usage of these libraries:\n",
    "\n",
    "1. `scipy.sparse`: for memory-efficient representation of ratings, vectorization of ratings manipulation.\n",
    "    1. `coo` matrices: easiest representation, consisting of three numpy arrays row, col, data; manipulating the data array leads to manipulation of the underlying values.\n",
    "    2. `csr` matrices: good for slicing rows; used for predicting ratings in item-based CF scenario.\n",
    "    3. `csc` matrices: good for slicing columns; used for predicting item ratings in user-based CF scenario.\n",
    "    4. `save_npz, load_npz` for checkpointing sparse matrices as files for fast loading.\n",
    "2. `sklearn`'s libraries.\n",
    "    1. `NearestNeighbors` from `sklearn.neighbors` for implementation of cosine similarity with baseline adjustments (whether adjust by mean or by a common number).\n",
    "    2. `KFold, train_test_split` from `sklearn.model_selection`. This will be used in conjunction with evaluation metrics.\n",
    "        1. Note: for `KFold, train_test_split`, this notebook implements splitting across \"users\" (not on ratings or items)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac595c76-bcd6-4d9d-8d26-eed3639ccc76",
   "metadata": {},
   "source": [
    "Specify where your `ratings.csv`, `books.csv` directories are.\n",
    "1. `ratings.csv` is the main data that we run CF on.\n",
    "2. `books.csv` is the data that we use for gaining more info on what are recommended by our recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "076c9f51-346d-4113-a788-ca0eebca1f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.02 s\n",
      "Wall time: 9.28 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>258</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4081</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>260</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  book_id  rating\n",
       "0        1      258       5\n",
       "1        2     4081       4\n",
       "2        2      260       5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "csv_dir = \"C:/Users/tydav/OneDrive/Documents/CSE6242/CSE6242_Project_Files/The-Last-Book-Bender/Data/Raw\" # edit this line\n",
    "ratings_all_df = pd.read_csv(\n",
    "    os.path.join(csv_dir, \"ratings.csv\"),\n",
    "    dtype={\n",
    "        \"user_id\": \"Int32\", # 50k users, so this is needed\n",
    "        \"book_id\": \"Int16\", # 10k books, so this is needed\n",
    "        \"rating\": \"Int8\"    # ranging from 0-5, so this is enough\n",
    "    }\n",
    ")\n",
    "ratings_all_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7803e64e-a14f-4dde-8428-5690c681f3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <td>5976479</td>\n",
       "      <td>1</td>\n",
       "      <td>53424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <td>5976479</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rating</th>\n",
       "      <td>5976479</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count  min    max\n",
       "user_id  5976479    1  53424\n",
       "book_id  5976479    1  10000\n",
       "rating   5976479    1      5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_all_df.describe().transpose()[[\"count\", \"min\", \"max\"]].astype(\"int32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4ad28d-e944-4abc-a1ae-bc20511dd5e2",
   "metadata": {},
   "source": [
    "so the dataframe has 10k books, 50k users, 6M books, with ratings given between 1-5. The corresponding user-item matrix (with rows the books and columns the users) has density `5976479/(53424*10000)=1.11%`.\n",
    "\n",
    "Using a sparse array for representing this dataframe can be nice. The dataframe is already a `coo` format; we directly build a `csc` matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0b724cd-2159-416d-93b1-233d194c11b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 93.8 ms\n",
      "Wall time: 308 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ratings_all_csc = sp.sparse.csc_matrix(\n",
    "    (\n",
    "        ratings_all_df[\"rating\"].astype(\"int32\"), # data\n",
    "        (ratings_all_df[\"book_id\"].astype(\"int32\"), ratings_all_df[\"user_id\"]) # (row, col)\n",
    "    )\n",
    ") # or csr. since we will slice on columns (users), csc is sensible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab9565c-615e-4364-8880-7239f5a4bd58",
   "metadata": {},
   "source": [
    "You can save this as an `npz` object using `save_npz`, and load it using `load_npz` (which is extremely fast)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3ba821-91ea-4e52-8a0e-e438dfd61f7e",
   "metadata": {},
   "source": [
    "## 2. Doing train test split for CV (on users)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ebbf5f-7373-4cfe-9b3d-bf6ca2d239b9",
   "metadata": {},
   "source": [
    "Since we will be evaluating models by k-fold cross validation, and out splitting is done on users, we first see how we can utilize the `KFold` function and check the speed of column slicing `csc` matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "528a8381-fdd0-4739-ae6d-c06b05a912c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting for fold 0 with train size 48082, test size 5343\n",
      "Splitting for fold 1 with train size 48082, test size 5343\n",
      "Splitting for fold 2 with train size 48082, test size 5343\n",
      "Splitting for fold 3 with train size 48082, test size 5343\n",
      "Splitting for fold 4 with train size 48082, test size 5343\n",
      "Splitting for fold 5 with train size 48083, test size 5342\n",
      "Splitting for fold 6 with train size 48083, test size 5342\n",
      "Splitting for fold 7 with train size 48083, test size 5342\n",
      "Splitting for fold 8 with train size 48083, test size 5342\n",
      "Splitting for fold 9 with train size 48083, test size 5342\n",
      "CPU times: total: 93.8 ms\n",
      "Wall time: 150 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=6242) # change this for other CV options\n",
    "user_ids = np.arange(53424 + 1) # the column range of our sparse matrix is 0-53424\n",
    "for fold, (user_ids_train, user_ids_test) in enumerate(kf.split(user_ids)):\n",
    "    print(f\"Splitting for fold {fold} with train size {len(user_ids_train)}, test size {len(user_ids_test)}\")\n",
    "    true_size = (10000 + 1, 53424 + 1)\n",
    "    ratings_train_csc = ratings_all_csc[:, user_ids_train]\n",
    "    ratings_test_csc = ratings_all_csc[:, user_ids_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61834aa-2847-43b0-9113-b4b3311306d9",
   "metadata": {},
   "source": [
    "Side note: if we use `csr` matrices (try changing the `tocsc` to `tocsr` one of the previous block), the time needed for the above block will be 8-9x.\n",
    "\n",
    "For demonstration purposes, we will be using the ninth fold (the last one) to show how set up CF. We will also make extensive use of the `user_ids_train` and `user_ids_test`, as converting from `csc` to `coo` will result in losing empty columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989c7be2-c363-4af6-9004-e3013a957ea8",
   "metadata": {},
   "source": [
    "## 3.1. Collaborative Filtering - Shifting Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471f397c-8a42-46fc-b173-6d810292f43f",
   "metadata": {},
   "source": [
    "The ratings are given from 1-5. We would guess that 3 = meh, 1 = terrible, 2 = bad, while 4, 5 indicates positive preference.\n",
    "\n",
    "There are several CF metrics for similarities of users. Given two rating vectors $u,v$, with mean $\\mu_u,\\mu_v$ (taken over nonzero entries; some prefer to talk about non-null entries if zero ratings are present), and support $I_u,I_v$ (where $I_u$ is set of indices of $u$ with nonzero entries):\n",
    "\n",
    "1. **Cosine similarity**: $(u\\cdot v)/(\\|u\\|\\|v\\|)$\n",
    "2. **Adjusted cosine similarity**: $(u'\\cdot v')/(\\|u'\\|\\|v'\\|)$ where $u'_i=\\begin{cases}u'_i-\\mu_u&\\text{if }i\\in I_u\\\\0\\end{cases}$; similarly for $v'$.\n",
    "3. **Pearson correlation**: $(u''\\cdot v'')/(\\|u''\\|\\|v''\\|)$ where $u''_i=\\begin{cases}u'_i&\\text{if }i\\in I_u\\cap I_v\\\\0\\end{cases}$; similarly for $v''$.\n",
    "4. **Pearson correlation with inverse user frequency**: $(u'''\\cdot v''')/(\\|u'''\\|\\|v'''\\|)$ where $u'''_i=\\sqrt{\\log(m/m_i)}u''_i$, with $m$ being the number of users, $m_i$ being the number of users rating an item $i$.\n",
    "5. Generalization of 2: take $u'_i=\\begin{cases}f(u_i)&\\text{if }i\\in I_u\\\\0\\end{cases}$ for some function $f$. Intuitively, $f(u_i)$ is a transformation of ratings that is only dependent on the value of ratings.\n",
    "   * For this notebook, we will test with the function $f(u_i)=u_i-2.9$, which makes $1, 2$ negative, and $3, 4, 5$ positive. We call this an **Adjusted Baseline**\n",
    "\n",
    "We will consider 1, 2, 5. We won't consider 3, 4, as it is a bit more complicated (`sklearn`'s `NearestNeighbors` will not be enough in this case (not supported for sparse inputs)).\n",
    "\n",
    "To do 5., we can simply modify the underlying data array (these are ordinary `numpy` arrays):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81eea0c0-4b7d-4068-8370-4b2fd5b5922c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 68.3 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.5, 2.5, 2.5, ..., 2.5, 2.5, 2.5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ratings_train_csc_demo = ratings_train_csc.copy().astype(np.float64) # no references is shared for this copy method\n",
    "mapping = np.array([0, -1.5, -0.5, 0.5, 1.5, 2.5])\n",
    "ratings_train_csc_demo.data = mapping[ratings_train_csc.data.astype(np.int8)]\n",
    "# alternatively, can use: ratings_train_csc_demo.data = ratings_train_csc.data - 2.5\n",
    "ratings_train_csc.data - ratings_train_csc_demo.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e469ef6-b9cd-4deb-8b67-04de15e9ca74",
   "metadata": {},
   "source": [
    "To do 2., the process involves counting number of nonzero entries in each column, sum along column, divide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17e9c075-0729-45d0-901e-f86090a7a29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 78.1 ms\n",
      "Wall time: 242 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ratings_train_csc_demo = ratings_train_csc.copy().astype(np.float64)\n",
    "ratings_train_csc_demo_col = ratings_train_csc_demo.tocoo().col\n",
    "col_sums = np.array(ratings_train_csc_demo.sum(axis=0)).flatten() # .sum(axis=0) gives np.matrix\n",
    "col_nonzero_counts = np.array(ratings_train_csc_demo.minimum(1).sum(axis=0)).flatten()\n",
    "ratings_train_csc_demo.data -= col_sums[ratings_train_csc_demo_col] / np.maximum(col_nonzero_counts[ratings_train_csc_demo_col], 0.5)\n",
    "# 0.5 is to avoid DivisionByZero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ae16bd-9e2f-4d02-ba38-a97af7f27300",
   "metadata": {},
   "source": [
    "These functions are abstracted as functions follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27b854b7-15e8-49a5-8ba6-d896ba337b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilate(X: sp.sparse.csc_matrix, amount: float) -> None:\n",
    "    \"\"\"\n",
    "    deduct each nonempty entry of X by amount\n",
    "    (this mutates the underlying csc matrix)\n",
    "    for example, take a as baseline.\n",
    "    \"\"\"\n",
    "    X.data -= np.float64(amount)\n",
    "\n",
    "def value_remap(X: sp.sparse.csc_matrix, remapper: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    remap nonnull values of X based on remapper\n",
    "    (if remapper[i]=j, value of i is remapped to j)\n",
    "    since indexing array only take integers, this might give unexpected errors\n",
    "    \"\"\"\n",
    "    X.data = remapper[X.data.astype(int)].astype(np.float64)\n",
    "\n",
    "def make_mean_0(X: sp.sparse.csc_matrix) -> None:\n",
    "    \"\"\"\n",
    "    make columns of a csc_matrix have zero mean\n",
    "    (this function mutates the underlying data)\n",
    "    \"\"\"\n",
    "    X.data -= get_true_mean(X)\n",
    "\n",
    "def get_true_mean(X: sp.sparse.csc_matrix) -> None:\n",
    "    \"\"\"\n",
    "    compute mean of each column (over nonzero indices!)\n",
    "    this won't mutate the underlying X\n",
    "    \n",
    "    \"\"\"\n",
    "    indexer = X.tocoo().col\n",
    "    v = np.array(X.sum(axis=0)).flatten()\n",
    "    c = np.array(X.minimum(1).sum(axis=0)).flatten()\n",
    "    return v[indexer] / np.maximum(c[indexer], 0.5)\n",
    "\n",
    "def test_mean_is_0(X: sp.sparse.csc_matrix, tol=10 ** (-10)) -> np.bool_:\n",
    "    \"\"\"\n",
    "    test if mean is 0 for each column\n",
    "    should be true on outputs of make_mean_0\n",
    "    \"\"\"\n",
    "    return np.abs(np.array(X.sum(axis=0))).max() < tol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0673363a-a008-4b63-b098-26f46bbccc5f",
   "metadata": {},
   "source": [
    "Sample usage (should be equivalent to above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eca11d5b-c416-4b33-bd2e-50d641ab4129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 125 ms\n",
      "Wall time: 298 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# need type conversion from int8; this conversion won't change row, col's dtype\n",
    "ratings_train_csc_demo = ratings_train_csc.copy().astype(np.float64)\n",
    "value_remap(ratings_train_csc_demo, np.array([0, -1.5, -0.5, 0.5, 1.5, 2.5]))\n",
    "dilate(ratings_train_csc_demo, -2.5)\n",
    "make_mean_0(ratings_train_csc_demo)\n",
    "# you can test if make_mean_0 is correct by uncommenting the next line\n",
    "test_mean_is_0(ratings_train_csc_demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9c583c-0d4e-4191-9ed3-9d0b83169f0d",
   "metadata": {},
   "source": [
    "## 3.2 Collaborative Filtering - Getting Top Similarity Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8bf096-61d3-4f66-87cf-4d931cb0f3a1",
   "metadata": {},
   "source": [
    "With these functions, we can plug these into `NearestNeighbors` to get top recommendations based on our desired metrics.\n",
    "\n",
    "Take adjusted cosine as example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e925ee1-058a-4424-8224-90d9927a5059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4.89 s\n",
      "Wall time: 13.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((5342, 5), (5342, 5))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# preprocess the training set\n",
    "ratings_train_csc_demo = ratings_train_csc\\\n",
    "                            .copy()\\\n",
    "                            .astype(np.float64)\n",
    "make_mean_0(ratings_train_csc_demo)\n",
    "# similarly for testing\n",
    "ratings_test_csc_demo = ratings_test_csc\\\n",
    "                            .copy()\\\n",
    "                            .astype(np.float64)\n",
    "# transform data\n",
    "make_mean_0(ratings_test_csc_demo)\n",
    "# fit knn\n",
    "n_neighbors = 5\n",
    "knn = NearestNeighbors(metric=\"cosine\")\n",
    "knn.fit(ratings_train_csc_demo.transpose()) # sklearn's convention is to use rows\n",
    "neigh_dist, neigh_ind = knn.kneighbors(X=ratings_test_csc_demo.transpose(), n_neighbors=n_neighbors) # this step took ~12 seconds on my machine\n",
    "neigh_dist.shape, neigh_ind.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7202ebef-0291-4631-a980-bea93dfac9e9",
   "metadata": {},
   "source": [
    "First row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec0b833d-cdee-41f7-8020-47100f3dcae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.6714676 , 0.79027302, 0.8244908 , 0.83439716, 0.84223844]),\n",
       " array([   24, 38041, 33316, 32243, 32773], dtype=int64))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh_dist[1, ], neigh_ind[1, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd79cd4f-9e56-4a76-b89a-c80809aca466",
   "metadata": {},
   "source": [
    "One gives indices, one gives distances. Distances are cosine distances - which is defined by $1-u\\cdot v/(\\|u\\|\\|v\\|)$. The similarity score can be recoverd as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "348ac15a-2e13-4f2b-9dd9-9143a6870a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_scores = 1 - neigh_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92364b54-c953-423c-9530-76a01dd6d87b",
   "metadata": {},
   "source": [
    "As a reality check, we can count number of `0`'s in the similarity scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9195f2bf-d1e1-4e03-bd8f-a4153736940e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(sim_scores[:, 0] == 0, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87e0c6c-cf2e-4c19-a393-988be41e787a",
   "metadata": {},
   "source": [
    "This is pretty ideal - only 25 people found no one in training column with positive similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6239e8-6996-47dc-b2a8-ca6c6c91944c",
   "metadata": {},
   "source": [
    "## 3.3 Collaborative Filtering - Predict Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3aa276-c729-47be-a87e-076d18b8df07",
   "metadata": {},
   "source": [
    "Now we have rankings and similarity scores. To evaluate model performance - such as MAP, MSE - we need to predict ratings on test user's ratings. We only evaluate on those with ground truths.\n",
    "\n",
    "Fix a number $k$ for desired neighborhood size. The formula (here $u$ is a user, $i$ is an item):\n",
    "\n",
    "$$\\hat{r}_{ui}=\\bar{r}_u+\\frac{\\sum_{v\\in N_{u,i}}\\text{sim}_{uv}(r_{vi}-\\bar{r}_v)}{\\sum_{v\\in N_{u,i}}|\\text{sim}_{uv}|},\\quad\\text{for test user }u,\\text{ an item }i\\text{ that }u\\text{ has rated}$$\n",
    "\n",
    "where $N_{u,i}$ is the set of top $k$ neighbors of $u$ **that also rated the item $u$**, $\\text{sim}_{uv}$ is similarity score between $u,v$, $r_{vi}$ is true rating. However, this means that $N_{u,i}$ is not only dependent on $u$ but also on $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc26779c-9b68-43f6-a15f-e35706e22bef",
   "metadata": {},
   "source": [
    "An approximate one is:\n",
    "\n",
    "$$\\hat{r}_{ui}=\\bar{r}_u+\\frac{\\sum_{v\\in N_u}\\delta_{i, I_v}\\text{sim}_{uv}(r_{vi}-\\bar{r}_v)}{\\sum_{v\\in N_u}\\delta_{i, I_v}|\\text{sim}_{uv}|}$$\n",
    "\n",
    "where $N_{u}$ is the top $k$ neighbors of $u$, and $\\delta_{i,I_v}=0$ if $i\\in I_v$, otherwise $0$ (recall that $I_v$ is the set of items that $v$ has rated).\n",
    "\n",
    "In this regards, `neigh_ind, neigh_dist` gave us all the $\\text{sim}_{uv},N_u$ needed. The intuition is that if a rating of an item $i$ is missing from a good neighbor $v$ of $u$, we don't let $v$'s score contribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d3a742d-449d-41a2-ba77-a6d4f09316c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 46.9 ms\n",
      "Wall time: 168 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# compute vector of means\n",
    "ratings_train_csc_mean = get_true_mean(ratings_train_csc)\n",
    "ratings_test_csc_mean = get_true_mean(ratings_test_csc)\n",
    "# initialize same array\n",
    "ratings_test_csc_predicted = ratings_test_csc.copy()\n",
    "ratings_test_csc_predicted.data = np.zeros(len(ratings_test_csc.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebca117e-598b-480c-91bc-f21895c13c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.06 s\n",
      "Wall time: 5.08 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([2.19387178e-04, 7.13008330e-04, 1.85315685e-03, 7.50902479e-03,\n",
       "        4.22187357e-02, 1.46427645e-01, 1.22170404e-01, 2.27378190e-01,\n",
       "        1.40319707e-01, 1.58598648e-01, 7.12592824e-02, 4.66779463e-02,\n",
       "        1.87044190e-02, 1.07366756e-02, 3.43872782e-03, 1.24818008e-03]),\n",
       " array([-4. , -3.5, -3. , -2.5, -2. , -1.5, -1. , -0.5,  0. ,  0.5,  1. ,\n",
       "         1.5,  2. ,  2.5,  3. ,  3.5,  4. ]),\n",
       " <BarContainer object of 16 artists>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "m = ratings_test_csc.shape[0]\n",
    "start_ind = 0\n",
    "for i in np.unique(ratings_test_csc.tocoo().col): # 5k / n_folds loops\n",
    "    denom = np.zeros((m, 1))\n",
    "    numer = np.zeros((m, 1))\n",
    "    baseline_items = ratings_test_csc[:, i].tocoo().row\n",
    "    n_baseline_items = len(baseline_items)\n",
    "    for j, sim_score in zip(neigh_ind[i], sim_scores[i]): # n_neighbors loops\n",
    "        r_v = ratings_train_csc[:, j].toarray()\n",
    "        mu_v = ratings_train_csc_mean[j]\n",
    "        w = sim_score * np.minimum(1, r_v)\n",
    "        denom += w\n",
    "        numer += w * (r_v - mu_v)\n",
    "    pred_ratings_for_one_user = (ratings_test_csc_mean[i] + numer / np.maximum(denom, 10 ** (-30)))[baseline_items].flatten()\n",
    "    ratings_test_csc_predicted.data[start_ind: start_ind + n_baseline_items] = pred_ratings_for_one_user\n",
    "    start_ind += n_baseline_items\n",
    "# visualize\n",
    "ratings_diffs = ratings_test_csc_predicted.data - ratings_test_csc.data\n",
    "n_ratings_test = ratings_test_csc_predicted.data.shape[0]\n",
    "plt.title(\"histogram of differencs between prediction and true ratings\")\n",
    "(counts, bins) = np.histogram(ratings_diffs, range=(-4, 4), bins=16)\n",
    "plt.hist(bins[:-1], bins, weights=counts/n_ratings_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dd991e-ab1a-4790-945d-f32839d64857",
   "metadata": {},
   "source": [
    "This plot shows a histogram of difference between our predicted ratings and the true ratings.\n",
    "\n",
    "We can also take a look at cumulative percentage of ratings being off:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93abbd0c-ae15-49fb-8ecb-a653eeb235b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Percentage of ratings being within a range (for every 0.5)')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ratings_diffs_abs = np.abs(ratings_diffs)\n",
    "(counts, bins) = np.histogram(ratings_diffs_abs, range=(0, 5), bins=10)\n",
    "plt.plot(bins[1:], np.cumsum(counts/n_ratings_test), marker=\"*\")\n",
    "plt.yticks(np.cumsum(counts/n_ratings_test)[:5])\n",
    "plt.xticks(np.arange(0, 10) / 2)\n",
    "plt.title(\"Percentage of ratings being within a range (for every 0.5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6955d7a6-6bd2-484f-9af6-a1fdb34166e4",
   "metadata": {},
   "source": [
    "This says about 36.77% have predictions being off by less than 0.5, 64.85% being off by less than 1.0, 86.61% being off by 1.5, and so on.\n",
    "\n",
    "For future reference, we abstract the rating prediction part as a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "927ff21b-96c9-48d2-9e70-251807b066d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(ratings_train_csc, ratings_test_csc, sim_scores, neigh_ind):\n",
    "    # compute vector of means\n",
    "    ratings_train_csc_mean = get_true_mean(ratings_train_csc)\n",
    "    ratings_test_csc_mean = get_true_mean(ratings_test_csc)\n",
    "    # initialize same array\n",
    "    ratings_test_csc_predicted = ratings_test_csc.copy()\n",
    "    ratings_test_csc_predicted.data = np.zeros(len(ratings_test_csc.data))\n",
    "    m = ratings_test_csc.shape[0]\n",
    "    start_ind = 0\n",
    "    for i in np.unique(ratings_test_csc.tocoo().col):\n",
    "        denom = np.zeros((m, 1))\n",
    "        numer = np.zeros((m, 1))\n",
    "        baseline_items = ratings_test_csc[:, i].tocoo().row\n",
    "        n_baseline_items = len(baseline_items)\n",
    "        for j, sim_score in zip(neigh_ind[i], sim_scores[i]):\n",
    "            r_v = ratings_train_csc[:, j].toarray()\n",
    "            mu_v = ratings_train_csc_mean[j]\n",
    "            w = sim_score * np.minimum(1, r_v)\n",
    "            denom += w\n",
    "            numer += w * (r_v - mu_v)\n",
    "        pred_ratings_for_one_user = (\n",
    "            ratings_test_csc_mean[i] + numer / np.maximum(denom, 10 ** (-30))\n",
    "        )[baseline_items].flatten()\n",
    "        ratings_test_csc_predicted.data[start_ind: start_ind + n_baseline_items] = pred_ratings_for_one_user\n",
    "        start_ind += n_baseline_items\n",
    "    return ratings_test_csc_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269fd1df-9d89-4c1c-a894-be4221f981c3",
   "metadata": {},
   "source": [
    "## 3.4 Collaborative Filtering - Evaluating ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84c3354-b7be-416b-8cc2-1ed4fea1d3e2",
   "metadata": {},
   "source": [
    "Let $r_i$ be the true user rating, $\\hat{r}_i$ be the predicted user rating from our collaborative filtering model, and n be the total number of ratings. We evaluate our model's performance using the following metrics:\n",
    "\n",
    "- $\\text{Mean Squared Error (MSE)} = \\frac{\\sum_{i=1}^n (r_i - \\hat{r}_i)^2}{n}$\n",
    "\n",
    "- $\\text{Root Mean Squared Error (RMSE)} = \\sqrt{\\frac{\\sum_{i=1}^n (r_i - \\hat{r}_i)^2}{n}} = \\sqrt{MSE}$\n",
    "\n",
    "- $\\text{Mean Absolute Error (MAE)} = \\frac{\\sum_{i=1}^n \\left| r_i - \\hat{r}_i \\right|}{n}$\n",
    "\n",
    "- $\\text{Mean Absolute Percentage Error (MAPE)} = \\sum_{i=1}^n \\frac{\\left| r_i - \\hat{r}_i \\right|}{n\\left| r_i \\right|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cde8bc97-ccf1-4a25-9088-8abcfeae7923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (Mean Square Error): 1.078309893943621\n",
      "RMSE (root mean square error): 1.038417013508351\n",
      "MAE (mean absolute error): 0.8331371945569175\n",
      "Mean Absolute Percentage Error: 0.26352744025030816\n"
     ]
    }
   ],
   "source": [
    "def eval_error(ratings_diffs: np.ndarray, sense:str=\"RMSE\"):\n",
    "    \"\"\"\n",
    "    function for evaluating MSE, RMSE, MAE, MAPE of a ratings_diffs array\n",
    "    \"\"\"\n",
    "    if sense not in {\"MSE\", \"RMSE\", \"MAE\", \"MAPE\"}:\n",
    "        raise NotImplementedError\n",
    "    elif sense in {\"RMSE\", \"MAE\"}:\n",
    "        p = {\"RMSE\": 2, \"MAE\": 1}[sense]\n",
    "        return np.linalg.norm(ratings_diffs, p) / ratings_diffs.shape[0] ** (1 / p)\n",
    "    elif sense == \"MSE\":\n",
    "        return ((ratings_diffs)**2).sum()/ratings_diffs.shape[0]\n",
    "    else:\n",
    "        return (np.abs(ratings_diffs)/np.abs(ratings_test_csc.data)).sum()/ratings_diffs.shape[0]\n",
    "\n",
    "print(f'MSE (Mean Square Error): {eval_error(ratings_diffs, \"MSE\")}')\n",
    "print(f'RMSE (root mean square error): {eval_error(ratings_diffs, \"RMSE\")}')\n",
    "print(f'MAE (mean absolute error): {eval_error(ratings_diffs, \"MAE\")}')\n",
    "print(f'Mean Absolute Percentage Error: {eval_error(ratings_diffs, \"MAPE\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf54fab4-7cd8-4165-8ccf-bc115ce92484",
   "metadata": {},
   "source": [
    "We can probably do better than this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804a48fe-8c98-448b-b228-0781edb86502",
   "metadata": {},
   "source": [
    "## 3.5 Collaborative Filtering - Putting all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7828a77b-fdc1-4c1b-a347-911f10354324",
   "metadata": {},
   "source": [
    "Let us make clear on which models that we want to try:\n",
    "1. Number of folds for CV - we fix this to be 10 for our task.\n",
    "2. Metrics to use - we use shifting by 0 (ordinary cosine), 2.5, 2.75, 2.9, 3, adjusted cosine, and the mapping of 1-2 to -1, 3-5 to 1.\n",
    "3. Number of neighborhoods to use - we use 3, 5, 10, 12, 15, 20, 30 (note that the time neede to generate predictions is roughly linear to number of neighborhoods used)\n",
    "\n",
    "The evaluation pipeline:\n",
    "* (A) Load data and convert to sparse matrix\n",
    "* (B) For each train test split index for a 10 fold CV:\n",
    "    * (B1) Build train test set (by sparse array column slicing)\n",
    "    * (B2) For every metrics that we want to use:\n",
    "        * (B2-1) Copy train test set and modify its values\n",
    "        * (B2-2) Build knn model from train set \n",
    "        * (B2-3) Fit this model to test set to grab 30 nearest neighbors (will time this step) and simimilarity scores.\n",
    "        * (B2-4) For for each number of neighborhoods we want to try:\n",
    "            * (B2-4-1) Get predicted scores for each user-item in train set (will time this step).\n",
    "            * (B2-4-2) Get difference of ratings (predicted versus baseline in train set)\n",
    "            * (B2-4-3) Calculate MSE, MAE, MAPE, and RMSE; perhaps also generate a histogram plot.\n",
    "            * (B2-4-4) Save MSE, MAE, MAPE, RMSE, prediction generation time.\n",
    "* (C) Combine (MSE, MAE, MAPE, RMSE, prediction generation time) for each pair of (metric, number of neighborhood) across the 10 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad6fcaee-f52f-4a12-8ce6-df4625a549a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold=0 started.\n",
      " - fold=0, metric='cosine' started.\n",
      " - - fold=0, metric='cosine', n_neighbors=3 started.\n",
      " - - fold=0, metric='cosine', n_neighbors=5 started.\n",
      " - - fold=0, metric='cosine', n_neighbors=10 started.\n",
      " - - fold=0, metric='cosine', n_neighbors=12 started.\n",
      " - - fold=0, metric='cosine', n_neighbors=15 started.\n",
      " - - fold=0, metric='cosine', n_neighbors=20 started.\n",
      " - - fold=0, metric='cosine', n_neighbors=30 started.\n",
      " - end of metric='cosine'; took 97.33994650840759 seconds.\n",
      " - fold=0, metric='deduct 2.5' started.\n",
      " - - fold=0, metric='deduct 2.5', n_neighbors=3 started.\n",
      " - - fold=0, metric='deduct 2.5', n_neighbors=5 started.\n",
      " - - fold=0, metric='deduct 2.5', n_neighbors=10 started.\n",
      " - - fold=0, metric='deduct 2.5', n_neighbors=12 started.\n",
      " - - fold=0, metric='deduct 2.5', n_neighbors=15 started.\n",
      " - - fold=0, metric='deduct 2.5', n_neighbors=20 started.\n",
      " - - fold=0, metric='deduct 2.5', n_neighbors=30 started.\n",
      " - end of metric='deduct 2.5'; took 94.1039412021637 seconds.\n",
      " - fold=0, metric='deduct 2.75' started.\n",
      " - - fold=0, metric='deduct 2.75', n_neighbors=3 started.\n",
      " - - fold=0, metric='deduct 2.75', n_neighbors=5 started.\n",
      " - - fold=0, metric='deduct 2.75', n_neighbors=10 started.\n",
      " - - fold=0, metric='deduct 2.75', n_neighbors=12 started.\n",
      " - - fold=0, metric='deduct 2.75', n_neighbors=15 started.\n",
      " - - fold=0, metric='deduct 2.75', n_neighbors=20 started.\n",
      " - - fold=0, metric='deduct 2.75', n_neighbors=30 started.\n",
      " - end of metric='deduct 2.75'; took 95.65493273735046 seconds.\n",
      " - fold=0, metric='deduct 2.9' started.\n",
      " - - fold=0, metric='deduct 2.9', n_neighbors=3 started.\n",
      " - - fold=0, metric='deduct 2.9', n_neighbors=5 started.\n",
      " - - fold=0, metric='deduct 2.9', n_neighbors=10 started.\n",
      " - - fold=0, metric='deduct 2.9', n_neighbors=12 started.\n",
      " - - fold=0, metric='deduct 2.9', n_neighbors=15 started.\n",
      " - - fold=0, metric='deduct 2.9', n_neighbors=20 started.\n",
      " - - fold=0, metric='deduct 2.9', n_neighbors=30 started.\n",
      " - end of metric='deduct 2.9'; took 95.34771156311035 seconds.\n",
      " - fold=0, metric='deduct 3' started.\n",
      " - - fold=0, metric='deduct 3', n_neighbors=3 started.\n",
      " - - fold=0, metric='deduct 3', n_neighbors=5 started.\n",
      " - - fold=0, metric='deduct 3', n_neighbors=10 started.\n",
      " - - fold=0, metric='deduct 3', n_neighbors=12 started.\n",
      " - - fold=0, metric='deduct 3', n_neighbors=15 started.\n",
      " - - fold=0, metric='deduct 3', n_neighbors=20 started.\n",
      " - - fold=0, metric='deduct 3', n_neighbors=30 started.\n",
      " - end of metric='deduct 3'; took 104.0982928276062 seconds.\n",
      " - fold=0, metric='adjusted cosine' started.\n",
      " - - fold=0, metric='adjusted cosine', n_neighbors=3 started.\n",
      " - - fold=0, metric='adjusted cosine', n_neighbors=5 started.\n",
      " - - fold=0, metric='adjusted cosine', n_neighbors=10 started.\n",
      " - - fold=0, metric='adjusted cosine', n_neighbors=12 started.\n",
      " - - fold=0, metric='adjusted cosine', n_neighbors=15 started.\n",
      " - - fold=0, metric='adjusted cosine', n_neighbors=20 started.\n",
      " - - fold=0, metric='adjusted cosine', n_neighbors=30 started.\n",
      " - end of metric='adjusted cosine'; took 95.3851900100708 seconds.\n",
      " - fold=0, metric='remap 12->-1, 345->1' started.\n",
      " - - fold=0, metric='remap 12->-1, 345->1', n_neighbors=3 started.\n",
      " - - fold=0, metric='remap 12->-1, 345->1', n_neighbors=5 started.\n",
      " - - fold=0, metric='remap 12->-1, 345->1', n_neighbors=10 started.\n",
      " - - fold=0, metric='remap 12->-1, 345->1', n_neighbors=12 started.\n",
      " - - fold=0, metric='remap 12->-1, 345->1', n_neighbors=15 started.\n",
      " - - fold=0, metric='remap 12->-1, 345->1', n_neighbors=20 started.\n",
      " - - fold=0, metric='remap 12->-1, 345->1', n_neighbors=30 started.\n",
      " - end of metric='remap 12->-1, 345->1'; took 96.82691383361816 seconds.\n",
      "end of fold-0; took 678.7588634490967 seconds.\n",
      "fold=1 started.\n",
      " - fold=1, metric='cosine' started.\n",
      " - - fold=1, metric='cosine', n_neighbors=3 started.\n",
      " - - fold=1, metric='cosine', n_neighbors=5 started.\n",
      " - - fold=1, metric='cosine', n_neighbors=10 started.\n",
      " - - fold=1, metric='cosine', n_neighbors=12 started.\n",
      " - - fold=1, metric='cosine', n_neighbors=15 started.\n",
      " - - fold=1, metric='cosine', n_neighbors=20 started.\n",
      " - - fold=1, metric='cosine', n_neighbors=30 started.\n",
      " - end of metric='cosine'; took 140.2583510875702 seconds.\n",
      " - fold=1, metric='deduct 2.5' started.\n",
      " - - fold=1, metric='deduct 2.5', n_neighbors=3 started.\n",
      " - - fold=1, metric='deduct 2.5', n_neighbors=5 started.\n",
      " - - fold=1, metric='deduct 2.5', n_neighbors=10 started.\n",
      " - - fold=1, metric='deduct 2.5', n_neighbors=12 started.\n",
      " - - fold=1, metric='deduct 2.5', n_neighbors=15 started.\n",
      " - - fold=1, metric='deduct 2.5', n_neighbors=20 started.\n",
      " - - fold=1, metric='deduct 2.5', n_neighbors=30 started.\n",
      " - end of metric='deduct 2.5'; took 132.46083688735962 seconds.\n",
      " - fold=1, metric='deduct 2.75' started.\n",
      " - - fold=1, metric='deduct 2.75', n_neighbors=3 started.\n",
      " - - fold=1, metric='deduct 2.75', n_neighbors=5 started.\n",
      " - - fold=1, metric='deduct 2.75', n_neighbors=10 started.\n",
      " - - fold=1, metric='deduct 2.75', n_neighbors=12 started.\n",
      " - - fold=1, metric='deduct 2.75', n_neighbors=15 started.\n",
      " - - fold=1, metric='deduct 2.75', n_neighbors=20 started.\n",
      " - - fold=1, metric='deduct 2.75', n_neighbors=30 started.\n",
      " - end of metric='deduct 2.75'; took 127.2028272151947 seconds.\n",
      " - fold=1, metric='deduct 2.9' started.\n",
      " - - fold=1, metric='deduct 2.9', n_neighbors=3 started.\n",
      " - - fold=1, metric='deduct 2.9', n_neighbors=5 started.\n",
      " - - fold=1, metric='deduct 2.9', n_neighbors=10 started.\n",
      " - - fold=1, metric='deduct 2.9', n_neighbors=12 started.\n",
      " - - fold=1, metric='deduct 2.9', n_neighbors=15 started.\n",
      " - - fold=1, metric='deduct 2.9', n_neighbors=20 started.\n",
      " - - fold=1, metric='deduct 2.9', n_neighbors=30 started.\n",
      " - end of metric='deduct 2.9'; took 153.92085599899292 seconds.\n",
      " - fold=1, metric='deduct 3' started.\n",
      " - - fold=1, metric='deduct 3', n_neighbors=3 started.\n",
      " - - fold=1, metric='deduct 3', n_neighbors=5 started.\n",
      " - - fold=1, metric='deduct 3', n_neighbors=10 started.\n",
      " - - fold=1, metric='deduct 3', n_neighbors=12 started.\n",
      " - - fold=1, metric='deduct 3', n_neighbors=15 started.\n",
      " - - fold=1, metric='deduct 3', n_neighbors=20 started.\n",
      " - - fold=1, metric='deduct 3', n_neighbors=30 started.\n",
      " - end of metric='deduct 3'; took 145.36699724197388 seconds.\n",
      " - fold=1, metric='adjusted cosine' started.\n",
      " - - fold=1, metric='adjusted cosine', n_neighbors=3 started.\n",
      " - - fold=1, metric='adjusted cosine', n_neighbors=5 started.\n",
      " - - fold=1, metric='adjusted cosine', n_neighbors=10 started.\n",
      " - - fold=1, metric='adjusted cosine', n_neighbors=12 started.\n",
      " - - fold=1, metric='adjusted cosine', n_neighbors=15 started.\n",
      " - - fold=1, metric='adjusted cosine', n_neighbors=20 started.\n",
      " - - fold=1, metric='adjusted cosine', n_neighbors=30 started.\n",
      " - end of metric='adjusted cosine'; took 148.5745232105255 seconds.\n",
      " - fold=1, metric='remap 12->-1, 345->1' started.\n",
      " - - fold=1, metric='remap 12->-1, 345->1', n_neighbors=3 started.\n",
      " - - fold=1, metric='remap 12->-1, 345->1', n_neighbors=5 started.\n",
      " - - fold=1, metric='remap 12->-1, 345->1', n_neighbors=10 started.\n",
      " - - fold=1, metric='remap 12->-1, 345->1', n_neighbors=12 started.\n",
      " - - fold=1, metric='remap 12->-1, 345->1', n_neighbors=15 started.\n",
      " - - fold=1, metric='remap 12->-1, 345->1', n_neighbors=20 started.\n",
      " - - fold=1, metric='remap 12->-1, 345->1', n_neighbors=30 started.\n",
      " - end of metric='remap 12->-1, 345->1'; took 168.10613489151 seconds.\n",
      "end of fold-1; took 1015.9037384986877 seconds.\n",
      "fold=2 started.\n",
      " - fold=2, metric='cosine' started.\n",
      " - - fold=2, metric='cosine', n_neighbors=3 started.\n",
      " - - fold=2, metric='cosine', n_neighbors=5 started.\n",
      " - - fold=2, metric='cosine', n_neighbors=10 started.\n",
      " - - fold=2, metric='cosine', n_neighbors=12 started.\n",
      " - - fold=2, metric='cosine', n_neighbors=15 started.\n",
      " - - fold=2, metric='cosine', n_neighbors=20 started.\n",
      " - - fold=2, metric='cosine', n_neighbors=30 started.\n",
      " - end of metric='cosine'; took 158.92054271697998 seconds.\n",
      " - fold=2, metric='deduct 2.5' started.\n",
      " - - fold=2, metric='deduct 2.5', n_neighbors=3 started.\n",
      " - - fold=2, metric='deduct 2.5', n_neighbors=5 started.\n",
      " - - fold=2, metric='deduct 2.5', n_neighbors=10 started.\n",
      " - - fold=2, metric='deduct 2.5', n_neighbors=12 started.\n",
      " - - fold=2, metric='deduct 2.5', n_neighbors=15 started.\n",
      " - - fold=2, metric='deduct 2.5', n_neighbors=20 started.\n",
      " - - fold=2, metric='deduct 2.5', n_neighbors=30 started.\n",
      " - end of metric='deduct 2.5'; took 98.90122151374817 seconds.\n",
      " - fold=2, metric='deduct 2.75' started.\n",
      " - - fold=2, metric='deduct 2.75', n_neighbors=3 started.\n",
      " - - fold=2, metric='deduct 2.75', n_neighbors=5 started.\n",
      " - - fold=2, metric='deduct 2.75', n_neighbors=10 started.\n",
      " - - fold=2, metric='deduct 2.75', n_neighbors=12 started.\n",
      " - - fold=2, metric='deduct 2.75', n_neighbors=15 started.\n",
      " - - fold=2, metric='deduct 2.75', n_neighbors=20 started.\n",
      " - - fold=2, metric='deduct 2.75', n_neighbors=30 started.\n",
      " - end of metric='deduct 2.75'; took 91.75513887405396 seconds.\n",
      " - fold=2, metric='deduct 2.9' started.\n",
      " - - fold=2, metric='deduct 2.9', n_neighbors=3 started.\n",
      " - - fold=2, metric='deduct 2.9', n_neighbors=5 started.\n",
      " - - fold=2, metric='deduct 2.9', n_neighbors=10 started.\n",
      " - - fold=2, metric='deduct 2.9', n_neighbors=12 started.\n",
      " - - fold=2, metric='deduct 2.9', n_neighbors=15 started.\n",
      " - - fold=2, metric='deduct 2.9', n_neighbors=20 started.\n",
      " - - fold=2, metric='deduct 2.9', n_neighbors=30 started.\n",
      " - end of metric='deduct 2.9'; took 92.01161241531372 seconds.\n",
      " - fold=2, metric='deduct 3' started.\n",
      " - - fold=2, metric='deduct 3', n_neighbors=3 started.\n",
      " - - fold=2, metric='deduct 3', n_neighbors=5 started.\n",
      " - - fold=2, metric='deduct 3', n_neighbors=10 started.\n",
      " - - fold=2, metric='deduct 3', n_neighbors=12 started.\n",
      " - - fold=2, metric='deduct 3', n_neighbors=15 started.\n",
      " - - fold=2, metric='deduct 3', n_neighbors=20 started.\n",
      " - - fold=2, metric='deduct 3', n_neighbors=30 started.\n",
      " - end of metric='deduct 3'; took 147.24745059013367 seconds.\n",
      " - fold=2, metric='adjusted cosine' started.\n",
      " - - fold=2, metric='adjusted cosine', n_neighbors=3 started.\n",
      " - - fold=2, metric='adjusted cosine', n_neighbors=5 started.\n",
      " - - fold=2, metric='adjusted cosine', n_neighbors=10 started.\n",
      " - - fold=2, metric='adjusted cosine', n_neighbors=12 started.\n",
      " - - fold=2, metric='adjusted cosine', n_neighbors=15 started.\n",
      " - - fold=2, metric='adjusted cosine', n_neighbors=20 started.\n",
      " - - fold=2, metric='adjusted cosine', n_neighbors=30 started.\n",
      " - end of metric='adjusted cosine'; took 91.92413854598999 seconds.\n",
      " - fold=2, metric='remap 12->-1, 345->1' started.\n",
      " - - fold=2, metric='remap 12->-1, 345->1', n_neighbors=3 started.\n",
      " - - fold=2, metric='remap 12->-1, 345->1', n_neighbors=5 started.\n",
      " - - fold=2, metric='remap 12->-1, 345->1', n_neighbors=10 started.\n",
      " - - fold=2, metric='remap 12->-1, 345->1', n_neighbors=12 started.\n",
      " - - fold=2, metric='remap 12->-1, 345->1', n_neighbors=15 started.\n",
      " - - fold=2, metric='remap 12->-1, 345->1', n_neighbors=20 started.\n",
      " - - fold=2, metric='remap 12->-1, 345->1', n_neighbors=30 started.\n",
      " - end of metric='remap 12->-1, 345->1'; took 91.54243087768555 seconds.\n",
      "end of fold-2; took 772.3302803039551 seconds.\n",
      "fold=3 started.\n",
      " - fold=3, metric='cosine' started.\n",
      " - - fold=3, metric='cosine', n_neighbors=3 started.\n",
      " - - fold=3, metric='cosine', n_neighbors=5 started.\n",
      " - - fold=3, metric='cosine', n_neighbors=10 started.\n",
      " - - fold=3, metric='cosine', n_neighbors=12 started.\n",
      " - - fold=3, metric='cosine', n_neighbors=15 started.\n",
      " - - fold=3, metric='cosine', n_neighbors=20 started.\n",
      " - - fold=3, metric='cosine', n_neighbors=30 started.\n",
      " - end of metric='cosine'; took 92.38134264945984 seconds.\n",
      " - fold=3, metric='deduct 2.5' started.\n",
      " - - fold=3, metric='deduct 2.5', n_neighbors=3 started.\n",
      " - - fold=3, metric='deduct 2.5', n_neighbors=5 started.\n",
      " - - fold=3, metric='deduct 2.5', n_neighbors=10 started.\n",
      " - - fold=3, metric='deduct 2.5', n_neighbors=12 started.\n",
      " - - fold=3, metric='deduct 2.5', n_neighbors=15 started.\n",
      " - - fold=3, metric='deduct 2.5', n_neighbors=20 started.\n",
      " - - fold=3, metric='deduct 2.5', n_neighbors=30 started.\n",
      " - end of metric='deduct 2.5'; took 1413.3748035430908 seconds.\n",
      " - fold=3, metric='deduct 2.75' started.\n",
      " - - fold=3, metric='deduct 2.75', n_neighbors=3 started.\n",
      " - - fold=3, metric='deduct 2.75', n_neighbors=5 started.\n",
      " - - fold=3, metric='deduct 2.75', n_neighbors=10 started.\n",
      " - - fold=3, metric='deduct 2.75', n_neighbors=12 started.\n",
      " - - fold=3, metric='deduct 2.75', n_neighbors=15 started.\n",
      " - - fold=3, metric='deduct 2.75', n_neighbors=20 started.\n",
      " - - fold=3, metric='deduct 2.75', n_neighbors=30 started.\n",
      " - end of metric='deduct 2.75'; took 103.30853486061096 seconds.\n",
      " - fold=3, metric='deduct 2.9' started.\n",
      " - - fold=3, metric='deduct 2.9', n_neighbors=3 started.\n",
      " - - fold=3, metric='deduct 2.9', n_neighbors=5 started.\n",
      " - - fold=3, metric='deduct 2.9', n_neighbors=10 started.\n",
      " - - fold=3, metric='deduct 2.9', n_neighbors=12 started.\n",
      " - - fold=3, metric='deduct 2.9', n_neighbors=15 started.\n",
      " - - fold=3, metric='deduct 2.9', n_neighbors=20 started.\n",
      " - - fold=3, metric='deduct 2.9', n_neighbors=30 started.\n",
      " - end of metric='deduct 2.9'; took 181.79287600517273 seconds.\n",
      " - fold=3, metric='deduct 3' started.\n",
      " - - fold=3, metric='deduct 3', n_neighbors=3 started.\n",
      " - - fold=3, metric='deduct 3', n_neighbors=5 started.\n",
      " - - fold=3, metric='deduct 3', n_neighbors=10 started.\n",
      " - - fold=3, metric='deduct 3', n_neighbors=12 started.\n",
      " - - fold=3, metric='deduct 3', n_neighbors=15 started.\n",
      " - - fold=3, metric='deduct 3', n_neighbors=20 started.\n",
      " - - fold=3, metric='deduct 3', n_neighbors=30 started.\n",
      " - end of metric='deduct 3'; took 209.79788780212402 seconds.\n",
      " - fold=3, metric='adjusted cosine' started.\n",
      " - - fold=3, metric='adjusted cosine', n_neighbors=3 started.\n",
      " - - fold=3, metric='adjusted cosine', n_neighbors=5 started.\n",
      " - - fold=3, metric='adjusted cosine', n_neighbors=10 started.\n",
      " - - fold=3, metric='adjusted cosine', n_neighbors=12 started.\n",
      " - - fold=3, metric='adjusted cosine', n_neighbors=15 started.\n",
      " - - fold=3, metric='adjusted cosine', n_neighbors=20 started.\n",
      " - - fold=3, metric='adjusted cosine', n_neighbors=30 started.\n",
      " - end of metric='adjusted cosine'; took 196.0205557346344 seconds.\n",
      " - fold=3, metric='remap 12->-1, 345->1' started.\n",
      " - - fold=3, metric='remap 12->-1, 345->1', n_neighbors=3 started.\n",
      " - - fold=3, metric='remap 12->-1, 345->1', n_neighbors=5 started.\n",
      " - - fold=3, metric='remap 12->-1, 345->1', n_neighbors=10 started.\n",
      " - - fold=3, metric='remap 12->-1, 345->1', n_neighbors=12 started.\n",
      " - - fold=3, metric='remap 12->-1, 345->1', n_neighbors=15 started.\n",
      " - - fold=3, metric='remap 12->-1, 345->1', n_neighbors=20 started.\n",
      " - - fold=3, metric='remap 12->-1, 345->1', n_neighbors=30 started.\n",
      " - end of metric='remap 12->-1, 345->1'; took 133.91170740127563 seconds.\n",
      "end of fold-3; took 2330.5953691005707 seconds.\n",
      "fold=4 started.\n",
      " - fold=4, metric='cosine' started.\n",
      " - - fold=4, metric='cosine', n_neighbors=3 started.\n",
      " - - fold=4, metric='cosine', n_neighbors=5 started.\n",
      " - - fold=4, metric='cosine', n_neighbors=10 started.\n",
      " - - fold=4, metric='cosine', n_neighbors=12 started.\n",
      " - - fold=4, metric='cosine', n_neighbors=15 started.\n",
      " - - fold=4, metric='cosine', n_neighbors=20 started.\n",
      " - - fold=4, metric='cosine', n_neighbors=30 started.\n",
      " - end of metric='cosine'; took 97.70601034164429 seconds.\n",
      " - fold=4, metric='deduct 2.5' started.\n",
      " - - fold=4, metric='deduct 2.5', n_neighbors=3 started.\n",
      " - - fold=4, metric='deduct 2.5', n_neighbors=5 started.\n",
      " - - fold=4, metric='deduct 2.5', n_neighbors=10 started.\n",
      " - - fold=4, metric='deduct 2.5', n_neighbors=12 started.\n",
      " - - fold=4, metric='deduct 2.5', n_neighbors=15 started.\n",
      " - - fold=4, metric='deduct 2.5', n_neighbors=20 started.\n",
      " - - fold=4, metric='deduct 2.5', n_neighbors=30 started.\n",
      " - end of metric='deduct 2.5'; took 98.58115673065186 seconds.\n",
      " - fold=4, metric='deduct 2.75' started.\n",
      " - - fold=4, metric='deduct 2.75', n_neighbors=3 started.\n",
      " - - fold=4, metric='deduct 2.75', n_neighbors=5 started.\n",
      " - - fold=4, metric='deduct 2.75', n_neighbors=10 started.\n",
      " - - fold=4, metric='deduct 2.75', n_neighbors=12 started.\n",
      " - - fold=4, metric='deduct 2.75', n_neighbors=15 started.\n",
      " - - fold=4, metric='deduct 2.75', n_neighbors=20 started.\n",
      " - - fold=4, metric='deduct 2.75', n_neighbors=30 started.\n",
      " - end of metric='deduct 2.75'; took 97.49645686149597 seconds.\n",
      " - fold=4, metric='deduct 2.9' started.\n",
      " - - fold=4, metric='deduct 2.9', n_neighbors=3 started.\n",
      " - - fold=4, metric='deduct 2.9', n_neighbors=5 started.\n",
      " - - fold=4, metric='deduct 2.9', n_neighbors=10 started.\n",
      " - - fold=4, metric='deduct 2.9', n_neighbors=12 started.\n",
      " - - fold=4, metric='deduct 2.9', n_neighbors=15 started.\n",
      " - - fold=4, metric='deduct 2.9', n_neighbors=20 started.\n",
      " - - fold=4, metric='deduct 2.9', n_neighbors=30 started.\n",
      " - end of metric='deduct 2.9'; took 96.58834099769592 seconds.\n",
      " - fold=4, metric='deduct 3' started.\n",
      " - - fold=4, metric='deduct 3', n_neighbors=3 started.\n",
      " - - fold=4, metric='deduct 3', n_neighbors=5 started.\n",
      " - - fold=4, metric='deduct 3', n_neighbors=10 started.\n",
      " - - fold=4, metric='deduct 3', n_neighbors=12 started.\n",
      " - - fold=4, metric='deduct 3', n_neighbors=15 started.\n",
      " - - fold=4, metric='deduct 3', n_neighbors=20 started.\n",
      " - - fold=4, metric='deduct 3', n_neighbors=30 started.\n",
      " - end of metric='deduct 3'; took 147.4323375225067 seconds.\n",
      " - fold=4, metric='adjusted cosine' started.\n",
      " - - fold=4, metric='adjusted cosine', n_neighbors=3 started.\n",
      " - - fold=4, metric='adjusted cosine', n_neighbors=5 started.\n",
      " - - fold=4, metric='adjusted cosine', n_neighbors=10 started.\n",
      " - - fold=4, metric='adjusted cosine', n_neighbors=12 started.\n",
      " - - fold=4, metric='adjusted cosine', n_neighbors=15 started.\n",
      " - - fold=4, metric='adjusted cosine', n_neighbors=20 started.\n",
      " - - fold=4, metric='adjusted cosine', n_neighbors=30 started.\n",
      " - end of metric='adjusted cosine'; took 190.10541915893555 seconds.\n",
      " - fold=4, metric='remap 12->-1, 345->1' started.\n",
      " - - fold=4, metric='remap 12->-1, 345->1', n_neighbors=3 started.\n",
      " - - fold=4, metric='remap 12->-1, 345->1', n_neighbors=5 started.\n",
      " - - fold=4, metric='remap 12->-1, 345->1', n_neighbors=10 started.\n",
      " - - fold=4, metric='remap 12->-1, 345->1', n_neighbors=12 started.\n",
      " - - fold=4, metric='remap 12->-1, 345->1', n_neighbors=15 started.\n",
      " - - fold=4, metric='remap 12->-1, 345->1', n_neighbors=20 started.\n",
      " - - fold=4, metric='remap 12->-1, 345->1', n_neighbors=30 started.\n",
      " - end of metric='remap 12->-1, 345->1'; took 203.94522619247437 seconds.\n",
      "end of fold-4; took 931.8714168071747 seconds.\n",
      "fold=5 started.\n",
      " - fold=5, metric='cosine' started.\n",
      " - - fold=5, metric='cosine', n_neighbors=3 started.\n",
      " - - fold=5, metric='cosine', n_neighbors=5 started.\n",
      " - - fold=5, metric='cosine', n_neighbors=10 started.\n",
      " - - fold=5, metric='cosine', n_neighbors=12 started.\n",
      " - - fold=5, metric='cosine', n_neighbors=15 started.\n",
      " - - fold=5, metric='cosine', n_neighbors=20 started.\n",
      " - - fold=5, metric='cosine', n_neighbors=30 started.\n",
      " - end of metric='cosine'; took 188.56244683265686 seconds.\n",
      " - fold=5, metric='deduct 2.5' started.\n",
      " - - fold=5, metric='deduct 2.5', n_neighbors=3 started.\n",
      " - - fold=5, metric='deduct 2.5', n_neighbors=5 started.\n",
      " - - fold=5, metric='deduct 2.5', n_neighbors=10 started.\n",
      " - - fold=5, metric='deduct 2.5', n_neighbors=12 started.\n",
      " - - fold=5, metric='deduct 2.5', n_neighbors=15 started.\n",
      " - - fold=5, metric='deduct 2.5', n_neighbors=20 started.\n",
      " - - fold=5, metric='deduct 2.5', n_neighbors=30 started.\n",
      " - end of metric='deduct 2.5'; took 157.16094946861267 seconds.\n",
      " - fold=5, metric='deduct 2.75' started.\n",
      " - - fold=5, metric='deduct 2.75', n_neighbors=3 started.\n",
      " - - fold=5, metric='deduct 2.75', n_neighbors=5 started.\n",
      " - - fold=5, metric='deduct 2.75', n_neighbors=10 started.\n",
      " - - fold=5, metric='deduct 2.75', n_neighbors=12 started.\n",
      " - - fold=5, metric='deduct 2.75', n_neighbors=15 started.\n",
      " - - fold=5, metric='deduct 2.75', n_neighbors=20 started.\n",
      " - - fold=5, metric='deduct 2.75', n_neighbors=30 started.\n",
      " - end of metric='deduct 2.75'; took 156.97155714035034 seconds.\n",
      " - fold=5, metric='deduct 2.9' started.\n",
      " - - fold=5, metric='deduct 2.9', n_neighbors=3 started.\n",
      " - - fold=5, metric='deduct 2.9', n_neighbors=5 started.\n",
      " - - fold=5, metric='deduct 2.9', n_neighbors=10 started.\n",
      " - - fold=5, metric='deduct 2.9', n_neighbors=12 started.\n",
      " - - fold=5, metric='deduct 2.9', n_neighbors=15 started.\n",
      " - - fold=5, metric='deduct 2.9', n_neighbors=20 started.\n",
      " - - fold=5, metric='deduct 2.9', n_neighbors=30 started.\n",
      " - end of metric='deduct 2.9'; took 158.33535742759705 seconds.\n",
      " - fold=5, metric='deduct 3' started.\n",
      " - - fold=5, metric='deduct 3', n_neighbors=3 started.\n",
      " - - fold=5, metric='deduct 3', n_neighbors=5 started.\n",
      " - - fold=5, metric='deduct 3', n_neighbors=10 started.\n",
      " - - fold=5, metric='deduct 3', n_neighbors=12 started.\n",
      " - - fold=5, metric='deduct 3', n_neighbors=15 started.\n",
      " - - fold=5, metric='deduct 3', n_neighbors=20 started.\n",
      " - - fold=5, metric='deduct 3', n_neighbors=30 started.\n",
      " - end of metric='deduct 3'; took 163.13681554794312 seconds.\n",
      " - fold=5, metric='adjusted cosine' started.\n",
      " - - fold=5, metric='adjusted cosine', n_neighbors=3 started.\n",
      " - - fold=5, metric='adjusted cosine', n_neighbors=5 started.\n",
      " - - fold=5, metric='adjusted cosine', n_neighbors=10 started.\n",
      " - - fold=5, metric='adjusted cosine', n_neighbors=12 started.\n",
      " - - fold=5, metric='adjusted cosine', n_neighbors=15 started.\n",
      " - - fold=5, metric='adjusted cosine', n_neighbors=20 started.\n",
      " - - fold=5, metric='adjusted cosine', n_neighbors=30 started.\n",
      " - end of metric='adjusted cosine'; took 165.73852705955505 seconds.\n",
      " - fold=5, metric='remap 12->-1, 345->1' started.\n",
      " - - fold=5, metric='remap 12->-1, 345->1', n_neighbors=3 started.\n",
      " - - fold=5, metric='remap 12->-1, 345->1', n_neighbors=5 started.\n",
      " - - fold=5, metric='remap 12->-1, 345->1', n_neighbors=10 started.\n",
      " - - fold=5, metric='remap 12->-1, 345->1', n_neighbors=12 started.\n",
      " - - fold=5, metric='remap 12->-1, 345->1', n_neighbors=15 started.\n",
      " - - fold=5, metric='remap 12->-1, 345->1', n_neighbors=20 started.\n",
      " - - fold=5, metric='remap 12->-1, 345->1', n_neighbors=30 started.\n",
      " - end of metric='remap 12->-1, 345->1'; took 96.62868762016296 seconds.\n",
      "end of fold-5; took 1086.5658974647522 seconds.\n",
      "fold=6 started.\n",
      " - fold=6, metric='cosine' started.\n",
      " - - fold=6, metric='cosine', n_neighbors=3 started.\n",
      " - - fold=6, metric='cosine', n_neighbors=5 started.\n",
      " - - fold=6, metric='cosine', n_neighbors=10 started.\n",
      " - - fold=6, metric='cosine', n_neighbors=12 started.\n",
      " - - fold=6, metric='cosine', n_neighbors=15 started.\n",
      " - - fold=6, metric='cosine', n_neighbors=20 started.\n",
      " - - fold=6, metric='cosine', n_neighbors=30 started.\n",
      " - end of metric='cosine'; took 95.9722170829773 seconds.\n",
      " - fold=6, metric='deduct 2.5' started.\n",
      " - - fold=6, metric='deduct 2.5', n_neighbors=3 started.\n",
      " - - fold=6, metric='deduct 2.5', n_neighbors=5 started.\n",
      " - - fold=6, metric='deduct 2.5', n_neighbors=10 started.\n",
      " - - fold=6, metric='deduct 2.5', n_neighbors=12 started.\n",
      " - - fold=6, metric='deduct 2.5', n_neighbors=15 started.\n",
      " - - fold=6, metric='deduct 2.5', n_neighbors=20 started.\n",
      " - - fold=6, metric='deduct 2.5', n_neighbors=30 started.\n",
      " - end of metric='deduct 2.5'; took 96.2887933254242 seconds.\n",
      " - fold=6, metric='deduct 2.75' started.\n",
      " - - fold=6, metric='deduct 2.75', n_neighbors=3 started.\n",
      " - - fold=6, metric='deduct 2.75', n_neighbors=5 started.\n",
      " - - fold=6, metric='deduct 2.75', n_neighbors=10 started.\n",
      " - - fold=6, metric='deduct 2.75', n_neighbors=12 started.\n",
      " - - fold=6, metric='deduct 2.75', n_neighbors=15 started.\n",
      " - - fold=6, metric='deduct 2.75', n_neighbors=20 started.\n",
      " - - fold=6, metric='deduct 2.75', n_neighbors=30 started.\n",
      " - end of metric='deduct 2.75'; took 96.15518999099731 seconds.\n",
      " - fold=6, metric='deduct 2.9' started.\n",
      " - - fold=6, metric='deduct 2.9', n_neighbors=3 started.\n",
      " - - fold=6, metric='deduct 2.9', n_neighbors=5 started.\n",
      " - - fold=6, metric='deduct 2.9', n_neighbors=10 started.\n",
      " - - fold=6, metric='deduct 2.9', n_neighbors=12 started.\n",
      " - - fold=6, metric='deduct 2.9', n_neighbors=15 started.\n",
      " - - fold=6, metric='deduct 2.9', n_neighbors=20 started.\n",
      " - - fold=6, metric='deduct 2.9', n_neighbors=30 started.\n",
      " - end of metric='deduct 2.9'; took 113.32336282730103 seconds.\n",
      " - fold=6, metric='deduct 3' started.\n",
      " - - fold=6, metric='deduct 3', n_neighbors=3 started.\n",
      " - - fold=6, metric='deduct 3', n_neighbors=5 started.\n",
      " - - fold=6, metric='deduct 3', n_neighbors=10 started.\n",
      " - - fold=6, metric='deduct 3', n_neighbors=12 started.\n",
      " - - fold=6, metric='deduct 3', n_neighbors=15 started.\n",
      " - - fold=6, metric='deduct 3', n_neighbors=20 started.\n",
      " - - fold=6, metric='deduct 3', n_neighbors=30 started.\n",
      " - end of metric='deduct 3'; took 193.84449458122253 seconds.\n",
      " - fold=6, metric='adjusted cosine' started.\n",
      " - - fold=6, metric='adjusted cosine', n_neighbors=3 started.\n",
      " - - fold=6, metric='adjusted cosine', n_neighbors=5 started.\n",
      " - - fold=6, metric='adjusted cosine', n_neighbors=10 started.\n",
      " - - fold=6, metric='adjusted cosine', n_neighbors=12 started.\n",
      " - - fold=6, metric='adjusted cosine', n_neighbors=15 started.\n",
      " - - fold=6, metric='adjusted cosine', n_neighbors=20 started.\n",
      " - - fold=6, metric='adjusted cosine', n_neighbors=30 started.\n",
      " - end of metric='adjusted cosine'; took 181.4547700881958 seconds.\n",
      " - fold=6, metric='remap 12->-1, 345->1' started.\n",
      " - - fold=6, metric='remap 12->-1, 345->1', n_neighbors=3 started.\n",
      " - - fold=6, metric='remap 12->-1, 345->1', n_neighbors=5 started.\n",
      " - - fold=6, metric='remap 12->-1, 345->1', n_neighbors=10 started.\n",
      " - - fold=6, metric='remap 12->-1, 345->1', n_neighbors=12 started.\n",
      " - - fold=6, metric='remap 12->-1, 345->1', n_neighbors=15 started.\n",
      " - - fold=6, metric='remap 12->-1, 345->1', n_neighbors=20 started.\n",
      " - - fold=6, metric='remap 12->-1, 345->1', n_neighbors=30 started.\n",
      " - end of metric='remap 12->-1, 345->1'; took 180.5358853340149 seconds.\n",
      "end of fold-6; took 957.5860121250153 seconds.\n",
      "fold=7 started.\n",
      " - fold=7, metric='cosine' started.\n",
      " - - fold=7, metric='cosine', n_neighbors=3 started.\n",
      " - - fold=7, metric='cosine', n_neighbors=5 started.\n",
      " - - fold=7, metric='cosine', n_neighbors=10 started.\n",
      " - - fold=7, metric='cosine', n_neighbors=12 started.\n",
      " - - fold=7, metric='cosine', n_neighbors=15 started.\n",
      " - - fold=7, metric='cosine', n_neighbors=20 started.\n",
      " - - fold=7, metric='cosine', n_neighbors=30 started.\n",
      " - end of metric='cosine'; took 195.0229263305664 seconds.\n",
      " - fold=7, metric='deduct 2.5' started.\n",
      " - - fold=7, metric='deduct 2.5', n_neighbors=3 started.\n",
      " - - fold=7, metric='deduct 2.5', n_neighbors=5 started.\n",
      " - - fold=7, metric='deduct 2.5', n_neighbors=10 started.\n",
      " - - fold=7, metric='deduct 2.5', n_neighbors=12 started.\n",
      " - - fold=7, metric='deduct 2.5', n_neighbors=15 started.\n",
      " - - fold=7, metric='deduct 2.5', n_neighbors=20 started.\n",
      " - - fold=7, metric='deduct 2.5', n_neighbors=30 started.\n",
      " - end of metric='deduct 2.5'; took 200.8303678035736 seconds.\n",
      " - fold=7, metric='deduct 2.75' started.\n",
      " - - fold=7, metric='deduct 2.75', n_neighbors=3 started.\n",
      " - - fold=7, metric='deduct 2.75', n_neighbors=5 started.\n",
      " - - fold=7, metric='deduct 2.75', n_neighbors=10 started.\n",
      " - - fold=7, metric='deduct 2.75', n_neighbors=12 started.\n",
      " - - fold=7, metric='deduct 2.75', n_neighbors=15 started.\n",
      " - - fold=7, metric='deduct 2.75', n_neighbors=20 started.\n",
      " - - fold=7, metric='deduct 2.75', n_neighbors=30 started.\n",
      " - end of metric='deduct 2.75'; took 194.41700506210327 seconds.\n",
      " - fold=7, metric='deduct 2.9' started.\n",
      " - - fold=7, metric='deduct 2.9', n_neighbors=3 started.\n",
      " - - fold=7, metric='deduct 2.9', n_neighbors=5 started.\n",
      " - - fold=7, metric='deduct 2.9', n_neighbors=10 started.\n",
      " - - fold=7, metric='deduct 2.9', n_neighbors=12 started.\n",
      " - - fold=7, metric='deduct 2.9', n_neighbors=15 started.\n",
      " - - fold=7, metric='deduct 2.9', n_neighbors=20 started.\n",
      " - - fold=7, metric='deduct 2.9', n_neighbors=30 started.\n",
      " - end of metric='deduct 2.9'; took 180.42306900024414 seconds.\n",
      " - fold=7, metric='deduct 3' started.\n",
      " - - fold=7, metric='deduct 3', n_neighbors=3 started.\n",
      " - - fold=7, metric='deduct 3', n_neighbors=5 started.\n",
      " - - fold=7, metric='deduct 3', n_neighbors=10 started.\n",
      " - - fold=7, metric='deduct 3', n_neighbors=12 started.\n",
      " - - fold=7, metric='deduct 3', n_neighbors=15 started.\n",
      " - - fold=7, metric='deduct 3', n_neighbors=20 started.\n",
      " - - fold=7, metric='deduct 3', n_neighbors=30 started.\n",
      " - end of metric='deduct 3'; took 175.58000707626343 seconds.\n",
      " - fold=7, metric='adjusted cosine' started.\n",
      " - - fold=7, metric='adjusted cosine', n_neighbors=3 started.\n",
      " - - fold=7, metric='adjusted cosine', n_neighbors=5 started.\n",
      " - - fold=7, metric='adjusted cosine', n_neighbors=10 started.\n",
      " - - fold=7, metric='adjusted cosine', n_neighbors=12 started.\n",
      " - - fold=7, metric='adjusted cosine', n_neighbors=15 started.\n",
      " - - fold=7, metric='adjusted cosine', n_neighbors=20 started.\n",
      " - - fold=7, metric='adjusted cosine', n_neighbors=30 started.\n",
      " - end of metric='adjusted cosine'; took 189.0206480026245 seconds.\n",
      " - fold=7, metric='remap 12->-1, 345->1' started.\n",
      " - - fold=7, metric='remap 12->-1, 345->1', n_neighbors=3 started.\n",
      " - - fold=7, metric='remap 12->-1, 345->1', n_neighbors=5 started.\n",
      " - - fold=7, metric='remap 12->-1, 345->1', n_neighbors=10 started.\n",
      " - - fold=7, metric='remap 12->-1, 345->1', n_neighbors=12 started.\n",
      " - - fold=7, metric='remap 12->-1, 345->1', n_neighbors=15 started.\n",
      " - - fold=7, metric='remap 12->-1, 345->1', n_neighbors=20 started.\n",
      " - - fold=7, metric='remap 12->-1, 345->1', n_neighbors=30 started.\n",
      " - end of metric='remap 12->-1, 345->1'; took 131.13979744911194 seconds.\n",
      "end of fold-7; took 1266.4636750221252 seconds.\n",
      "fold=8 started.\n",
      " - fold=8, metric='cosine' started.\n",
      " - - fold=8, metric='cosine', n_neighbors=3 started.\n",
      " - - fold=8, metric='cosine', n_neighbors=5 started.\n",
      " - - fold=8, metric='cosine', n_neighbors=10 started.\n",
      " - - fold=8, metric='cosine', n_neighbors=12 started.\n",
      " - - fold=8, metric='cosine', n_neighbors=15 started.\n",
      " - - fold=8, metric='cosine', n_neighbors=20 started.\n",
      " - - fold=8, metric='cosine', n_neighbors=30 started.\n",
      " - end of metric='cosine'; took 106.66021299362183 seconds.\n",
      " - fold=8, metric='deduct 2.5' started.\n",
      " - - fold=8, metric='deduct 2.5', n_neighbors=3 started.\n",
      " - - fold=8, metric='deduct 2.5', n_neighbors=5 started.\n",
      " - - fold=8, metric='deduct 2.5', n_neighbors=10 started.\n",
      " - - fold=8, metric='deduct 2.5', n_neighbors=12 started.\n",
      " - - fold=8, metric='deduct 2.5', n_neighbors=15 started.\n",
      " - - fold=8, metric='deduct 2.5', n_neighbors=20 started.\n",
      " - - fold=8, metric='deduct 2.5', n_neighbors=30 started.\n",
      " - end of metric='deduct 2.5'; took 95.48633217811584 seconds.\n",
      " - fold=8, metric='deduct 2.75' started.\n",
      " - - fold=8, metric='deduct 2.75', n_neighbors=3 started.\n",
      " - - fold=8, metric='deduct 2.75', n_neighbors=5 started.\n",
      " - - fold=8, metric='deduct 2.75', n_neighbors=10 started.\n",
      " - - fold=8, metric='deduct 2.75', n_neighbors=12 started.\n",
      " - - fold=8, metric='deduct 2.75', n_neighbors=15 started.\n",
      " - - fold=8, metric='deduct 2.75', n_neighbors=20 started.\n",
      " - - fold=8, metric='deduct 2.75', n_neighbors=30 started.\n",
      " - end of metric='deduct 2.75'; took 97.7730360031128 seconds.\n",
      " - fold=8, metric='deduct 2.9' started.\n",
      " - - fold=8, metric='deduct 2.9', n_neighbors=3 started.\n",
      " - - fold=8, metric='deduct 2.9', n_neighbors=5 started.\n",
      " - - fold=8, metric='deduct 2.9', n_neighbors=10 started.\n",
      " - - fold=8, metric='deduct 2.9', n_neighbors=12 started.\n",
      " - - fold=8, metric='deduct 2.9', n_neighbors=15 started.\n",
      " - - fold=8, metric='deduct 2.9', n_neighbors=20 started.\n",
      " - - fold=8, metric='deduct 2.9', n_neighbors=30 started.\n",
      " - end of metric='deduct 2.9'; took 96.16502261161804 seconds.\n",
      " - fold=8, metric='deduct 3' started.\n",
      " - - fold=8, metric='deduct 3', n_neighbors=3 started.\n",
      " - - fold=8, metric='deduct 3', n_neighbors=5 started.\n",
      " - - fold=8, metric='deduct 3', n_neighbors=10 started.\n",
      " - - fold=8, metric='deduct 3', n_neighbors=12 started.\n",
      " - - fold=8, metric='deduct 3', n_neighbors=15 started.\n",
      " - - fold=8, metric='deduct 3', n_neighbors=20 started.\n",
      " - - fold=8, metric='deduct 3', n_neighbors=30 started.\n",
      " - end of metric='deduct 3'; took 114.31622266769409 seconds.\n",
      " - fold=8, metric='adjusted cosine' started.\n",
      " - - fold=8, metric='adjusted cosine', n_neighbors=3 started.\n",
      " - - fold=8, metric='adjusted cosine', n_neighbors=5 started.\n",
      " - - fold=8, metric='adjusted cosine', n_neighbors=10 started.\n",
      " - - fold=8, metric='adjusted cosine', n_neighbors=12 started.\n",
      " - - fold=8, metric='adjusted cosine', n_neighbors=15 started.\n",
      " - - fold=8, metric='adjusted cosine', n_neighbors=20 started.\n",
      " - - fold=8, metric='adjusted cosine', n_neighbors=30 started.\n",
      " - end of metric='adjusted cosine'; took 157.1751103401184 seconds.\n",
      " - fold=8, metric='remap 12->-1, 345->1' started.\n",
      " - - fold=8, metric='remap 12->-1, 345->1', n_neighbors=3 started.\n",
      " - - fold=8, metric='remap 12->-1, 345->1', n_neighbors=5 started.\n",
      " - - fold=8, metric='remap 12->-1, 345->1', n_neighbors=10 started.\n",
      " - - fold=8, metric='remap 12->-1, 345->1', n_neighbors=12 started.\n",
      " - - fold=8, metric='remap 12->-1, 345->1', n_neighbors=15 started.\n",
      " - - fold=8, metric='remap 12->-1, 345->1', n_neighbors=20 started.\n",
      " - - fold=8, metric='remap 12->-1, 345->1', n_neighbors=30 started.\n",
      " - end of metric='remap 12->-1, 345->1'; took 150.21556067466736 seconds.\n",
      "end of fold-8; took 817.8231408596039 seconds.\n",
      "fold=9 started.\n",
      " - fold=9, metric='cosine' started.\n",
      " - - fold=9, metric='cosine', n_neighbors=3 started.\n",
      " - - fold=9, metric='cosine', n_neighbors=5 started.\n",
      " - - fold=9, metric='cosine', n_neighbors=10 started.\n",
      " - - fold=9, metric='cosine', n_neighbors=12 started.\n",
      " - - fold=9, metric='cosine', n_neighbors=15 started.\n",
      " - - fold=9, metric='cosine', n_neighbors=20 started.\n",
      " - - fold=9, metric='cosine', n_neighbors=30 started.\n",
      " - end of metric='cosine'; took 187.68803477287292 seconds.\n",
      " - fold=9, metric='deduct 2.5' started.\n",
      " - - fold=9, metric='deduct 2.5', n_neighbors=3 started.\n",
      " - - fold=9, metric='deduct 2.5', n_neighbors=5 started.\n",
      " - - fold=9, metric='deduct 2.5', n_neighbors=10 started.\n",
      " - - fold=9, metric='deduct 2.5', n_neighbors=12 started.\n",
      " - - fold=9, metric='deduct 2.5', n_neighbors=15 started.\n",
      " - - fold=9, metric='deduct 2.5', n_neighbors=20 started.\n",
      " - - fold=9, metric='deduct 2.5', n_neighbors=30 started.\n",
      " - end of metric='deduct 2.5'; took 188.5932490825653 seconds.\n",
      " - fold=9, metric='deduct 2.75' started.\n",
      " - - fold=9, metric='deduct 2.75', n_neighbors=3 started.\n",
      " - - fold=9, metric='deduct 2.75', n_neighbors=5 started.\n",
      " - - fold=9, metric='deduct 2.75', n_neighbors=10 started.\n",
      " - - fold=9, metric='deduct 2.75', n_neighbors=12 started.\n",
      " - - fold=9, metric='deduct 2.75', n_neighbors=15 started.\n",
      " - - fold=9, metric='deduct 2.75', n_neighbors=20 started.\n",
      " - - fold=9, metric='deduct 2.75', n_neighbors=30 started.\n",
      " - end of metric='deduct 2.75'; took 186.68640327453613 seconds.\n",
      " - fold=9, metric='deduct 2.9' started.\n",
      " - - fold=9, metric='deduct 2.9', n_neighbors=3 started.\n",
      " - - fold=9, metric='deduct 2.9', n_neighbors=5 started.\n",
      " - - fold=9, metric='deduct 2.9', n_neighbors=10 started.\n",
      " - - fold=9, metric='deduct 2.9', n_neighbors=12 started.\n",
      " - - fold=9, metric='deduct 2.9', n_neighbors=15 started.\n",
      " - - fold=9, metric='deduct 2.9', n_neighbors=20 started.\n",
      " - - fold=9, metric='deduct 2.9', n_neighbors=30 started.\n",
      " - end of metric='deduct 2.9'; took 126.21781659126282 seconds.\n",
      " - fold=9, metric='deduct 3' started.\n",
      " - - fold=9, metric='deduct 3', n_neighbors=3 started.\n",
      " - - fold=9, metric='deduct 3', n_neighbors=5 started.\n",
      " - - fold=9, metric='deduct 3', n_neighbors=10 started.\n",
      " - - fold=9, metric='deduct 3', n_neighbors=12 started.\n",
      " - - fold=9, metric='deduct 3', n_neighbors=15 started.\n",
      " - - fold=9, metric='deduct 3', n_neighbors=20 started.\n",
      " - - fold=9, metric='deduct 3', n_neighbors=30 started.\n",
      " - end of metric='deduct 3'; took 104.6411828994751 seconds.\n",
      " - fold=9, metric='adjusted cosine' started.\n",
      " - - fold=9, metric='adjusted cosine', n_neighbors=3 started.\n",
      " - - fold=9, metric='adjusted cosine', n_neighbors=5 started.\n",
      " - - fold=9, metric='adjusted cosine', n_neighbors=10 started.\n",
      " - - fold=9, metric='adjusted cosine', n_neighbors=12 started.\n",
      " - - fold=9, metric='adjusted cosine', n_neighbors=15 started.\n",
      " - - fold=9, metric='adjusted cosine', n_neighbors=20 started.\n",
      " - - fold=9, metric='adjusted cosine', n_neighbors=30 started.\n",
      " - end of metric='adjusted cosine'; took 98.33330225944519 seconds.\n",
      " - fold=9, metric='remap 12->-1, 345->1' started.\n",
      " - - fold=9, metric='remap 12->-1, 345->1', n_neighbors=3 started.\n",
      " - - fold=9, metric='remap 12->-1, 345->1', n_neighbors=5 started.\n",
      " - - fold=9, metric='remap 12->-1, 345->1', n_neighbors=10 started.\n",
      " - - fold=9, metric='remap 12->-1, 345->1', n_neighbors=12 started.\n",
      " - - fold=9, metric='remap 12->-1, 345->1', n_neighbors=15 started.\n",
      " - - fold=9, metric='remap 12->-1, 345->1', n_neighbors=20 started.\n",
      " - - fold=9, metric='remap 12->-1, 345->1', n_neighbors=30 started.\n",
      " - end of metric='remap 12->-1, 345->1'; took 158.6613266468048 seconds.\n",
      "end of fold-9; took 1050.8527944087982 seconds.\n",
      "CPU times: total: 1h 10min 24s\n",
      "Wall time: 3h 1min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = []\n",
    "# (A)\n",
    "csv_dir = \"C:/Users/tydav/OneDrive/Documents/CSE6242/CSE6242_Project_Files/The-Last-Book-Bender/Data/Raw\"\n",
    "ratings_all_df = pd.read_csv(\n",
    "    os.path.join(csv_dir, \"ratings.csv\"),\n",
    "    dtype={\n",
    "        \"user_id\": \"Int32\",\n",
    "        \"book_id\": \"Int16\",\n",
    "        \"rating\": \"Int8\"\n",
    "    }\n",
    ")\n",
    "ratings_all_csc = sp.sparse.csc_matrix(\n",
    "    (\n",
    "        ratings_all_df[\"rating\"].astype(\"int32\"), # data\n",
    "        (ratings_all_df[\"book_id\"].astype(\"int32\"), ratings_all_df[\"user_id\"]) # (row, col)\n",
    "    )\n",
    ")\n",
    "# (B)\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=6242)\n",
    "user_ids = np.arange(53424 + 1)\n",
    "for (fold, (user_ids_train, user_ids_test)) in enumerate(kf.split(user_ids)):\n",
    "    print(f\"fold={fold} started.\")\n",
    "    fold_time_start = time.time()\n",
    "    # (B1)\n",
    "    true_size = (10000 + 1, 53424 + 1)\n",
    "    ratings_train_csc = ratings_all_csc[:, user_ids_train]\n",
    "    ratings_test_csc = ratings_all_csc[:, user_ids_test]\n",
    "    for metric_name, func in zip(\n",
    "        [\n",
    "            \"cosine\", *[f\"deduct {amount}\" for amount in [2.5, 2.75, 2.9, 3]],\n",
    "            \"adjusted cosine\",\n",
    "            \"remap 12->-1, 345->1\"\n",
    "        ],\n",
    "        [\n",
    "            *[lambda x: dilate(x, amount) for amount in [0, 2.5, 2.75, 2.9, 3]],\n",
    "            make_mean_0,\n",
    "            lambda x: value_remap(x, np.array([0, -1, -1, 1, 1, 1]))\n",
    "        ]\n",
    "    ):\n",
    "        print(f\" - fold={fold}, metric='{metric_name}' started.\")\n",
    "        metric_time_start = time.time()\n",
    "        # (B2-1)\n",
    "        ratings_train_csc_copied = ratings_train_csc.copy().astype(np.float64)\n",
    "        ratings_test_csc_copied = ratings_test_csc.copy().astype(np.float64)\n",
    "        func(ratings_train_csc_copied)\n",
    "        func(ratings_test_csc_copied)\n",
    "        # (B2-2)\n",
    "        fit_time_start = time.time()\n",
    "        knn = NearestNeighbors(metric=\"cosine\")\n",
    "        knn.fit(ratings_train_csc_copied.transpose())\n",
    "        fit_time = time.time() - fit_time_start\n",
    "        # (B2-3)\n",
    "        neigh_dist, neigh_ind = knn.kneighbors(\n",
    "            X=ratings_test_csc_copied.transpose(),\n",
    "            n_neighbors=30\n",
    "        )\n",
    "        sim_scores = 1 - neigh_dist\n",
    "        # (B2-4)\n",
    "        for n_neighbors in [3, 5, 10, 12, 15, 20, 30]:\n",
    "            print(f\" - - fold={fold}, metric='{metric_name}', n_neighbors={n_neighbors} started.\")\n",
    "            # (B2-4-1)\n",
    "            predict_time_start = time.time()\n",
    "            ratings_test_csc_predicted = predict(\n",
    "                ratings_train_csc,\n",
    "                ratings_test_csc,\n",
    "                sim_scores[:, :n_neighbors],\n",
    "                neigh_ind[:, :n_neighbors]\n",
    "            )\n",
    "            predict_time = time.time() - predict_time_start\n",
    "            # (B2-4-2)\n",
    "            ratings_diffs = ratings_test_csc_predicted.data - ratings_test_csc.data\n",
    "            # (B2-4-3)\n",
    "            mse = eval_error(ratings_diffs, \"MSE\")\n",
    "            rmse = eval_error(ratings_diffs, \"RMSE\")\n",
    "            mae = eval_error(ratings_diffs, \"MAE\")\n",
    "            mape = eval_error(ratings_diffs, \"MAPE\")\n",
    "            # (B2-4-4)\n",
    "            results.append(\n",
    "                {\n",
    "                    \"fold\": fold,\n",
    "                    \"metric_name\": metric_name,\n",
    "                    \"n_neighbors\": n_neighbors,\n",
    "                    \"predict_time\": predict_time,\n",
    "                    \"fit_time\": fit_time,\n",
    "                    \"mse\": mse,\n",
    "                    \"rmse\": rmse,\n",
    "                    \"mae\": mae,\n",
    "                    \"mape\": mape\n",
    "                }\n",
    "            )\n",
    "        print(f\" - end of metric='{metric_name}'; took {time.time() - metric_time_start} seconds.\")\n",
    "    print(f\"end of fold-{fold}; took {time.time() - fold_time_start} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37bf96c5-c75b-40c2-88c1-f9231a24d4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('results_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "with open('results_dict.pkl', 'rb') as f:\n",
    "    test_pkl_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "665a77aa-0aaf-4e62-9a8d-f02838e5fe92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>mape</th>\n",
       "      <th>predict_time</th>\n",
       "      <th>fit_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metric_name</th>\n",
       "      <th>n_neighbors</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">adjusted cosine</th>\n",
       "      <th>3</th>\n",
       "      <td>1.113208</td>\n",
       "      <td>1.055023</td>\n",
       "      <td>0.848436</td>\n",
       "      <td>0.265783</td>\n",
       "      <td>50.160261</td>\n",
       "      <td>0.028253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.092028</td>\n",
       "      <td>1.044940</td>\n",
       "      <td>0.835864</td>\n",
       "      <td>0.261168</td>\n",
       "      <td>73.615602</td>\n",
       "      <td>0.028253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.041937</td>\n",
       "      <td>1.020688</td>\n",
       "      <td>0.810304</td>\n",
       "      <td>0.252746</td>\n",
       "      <td>136.973672</td>\n",
       "      <td>0.028253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.024822</td>\n",
       "      <td>1.012270</td>\n",
       "      <td>0.802031</td>\n",
       "      <td>0.250146</td>\n",
       "      <td>167.926390</td>\n",
       "      <td>0.028253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.002224</td>\n",
       "      <td>1.001044</td>\n",
       "      <td>0.791263</td>\n",
       "      <td>0.246783</td>\n",
       "      <td>218.771822</td>\n",
       "      <td>0.028253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.969780</td>\n",
       "      <td>0.984701</td>\n",
       "      <td>0.776419</td>\n",
       "      <td>0.242251</td>\n",
       "      <td>278.644197</td>\n",
       "      <td>0.028253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.921901</td>\n",
       "      <td>0.960081</td>\n",
       "      <td>0.754724</td>\n",
       "      <td>0.235772</td>\n",
       "      <td>401.716500</td>\n",
       "      <td>0.028253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">cosine</th>\n",
       "      <th>3</th>\n",
       "      <td>1.095035</td>\n",
       "      <td>1.046377</td>\n",
       "      <td>0.828041</td>\n",
       "      <td>0.272714</td>\n",
       "      <td>45.686521</td>\n",
       "      <td>0.021305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.076076</td>\n",
       "      <td>1.037272</td>\n",
       "      <td>0.816653</td>\n",
       "      <td>0.270006</td>\n",
       "      <td>69.210290</td>\n",
       "      <td>0.021305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.040506</td>\n",
       "      <td>1.019972</td>\n",
       "      <td>0.798785</td>\n",
       "      <td>0.265291</td>\n",
       "      <td>123.839174</td>\n",
       "      <td>0.021305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.030204</td>\n",
       "      <td>1.014907</td>\n",
       "      <td>0.794056</td>\n",
       "      <td>0.263976</td>\n",
       "      <td>147.952476</td>\n",
       "      <td>0.021305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.016827</td>\n",
       "      <td>1.008292</td>\n",
       "      <td>0.788054</td>\n",
       "      <td>0.262368</td>\n",
       "      <td>190.248673</td>\n",
       "      <td>0.021305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.999288</td>\n",
       "      <td>0.999552</td>\n",
       "      <td>0.780277</td>\n",
       "      <td>0.260245</td>\n",
       "      <td>254.897141</td>\n",
       "      <td>0.021305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.973432</td>\n",
       "      <td>0.986529</td>\n",
       "      <td>0.769471</td>\n",
       "      <td>0.257268</td>\n",
       "      <td>373.673951</td>\n",
       "      <td>0.021305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">deduct 2.5</th>\n",
       "      <th>3</th>\n",
       "      <td>1.095035</td>\n",
       "      <td>1.046377</td>\n",
       "      <td>0.828041</td>\n",
       "      <td>0.272714</td>\n",
       "      <td>41.657907</td>\n",
       "      <td>0.021487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.076076</td>\n",
       "      <td>1.037272</td>\n",
       "      <td>0.816653</td>\n",
       "      <td>0.270006</td>\n",
       "      <td>1272.386797</td>\n",
       "      <td>0.021487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.040506</td>\n",
       "      <td>1.019972</td>\n",
       "      <td>0.798785</td>\n",
       "      <td>0.265291</td>\n",
       "      <td>126.539474</td>\n",
       "      <td>0.021487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.030204</td>\n",
       "      <td>1.014907</td>\n",
       "      <td>0.794056</td>\n",
       "      <td>0.263976</td>\n",
       "      <td>174.205192</td>\n",
       "      <td>0.021487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.016827</td>\n",
       "      <td>1.008292</td>\n",
       "      <td>0.788054</td>\n",
       "      <td>0.262368</td>\n",
       "      <td>214.388090</td>\n",
       "      <td>0.021487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.999288</td>\n",
       "      <td>0.999552</td>\n",
       "      <td>0.780277</td>\n",
       "      <td>0.260245</td>\n",
       "      <td>270.670647</td>\n",
       "      <td>0.021487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.973432</td>\n",
       "      <td>0.986529</td>\n",
       "      <td>0.769471</td>\n",
       "      <td>0.257268</td>\n",
       "      <td>326.099410</td>\n",
       "      <td>0.021487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">deduct 2.75</th>\n",
       "      <th>3</th>\n",
       "      <td>1.095035</td>\n",
       "      <td>1.046377</td>\n",
       "      <td>0.828041</td>\n",
       "      <td>0.272714</td>\n",
       "      <td>41.850846</td>\n",
       "      <td>0.026044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.076076</td>\n",
       "      <td>1.037272</td>\n",
       "      <td>0.816653</td>\n",
       "      <td>0.270006</td>\n",
       "      <td>64.360751</td>\n",
       "      <td>0.026044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.040506</td>\n",
       "      <td>1.019972</td>\n",
       "      <td>0.798785</td>\n",
       "      <td>0.265291</td>\n",
       "      <td>115.235674</td>\n",
       "      <td>0.026044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.030204</td>\n",
       "      <td>1.014907</td>\n",
       "      <td>0.794056</td>\n",
       "      <td>0.263976</td>\n",
       "      <td>135.968183</td>\n",
       "      <td>0.026044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.016827</td>\n",
       "      <td>1.008292</td>\n",
       "      <td>0.788054</td>\n",
       "      <td>0.262368</td>\n",
       "      <td>169.061890</td>\n",
       "      <td>0.026044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.999288</td>\n",
       "      <td>0.999552</td>\n",
       "      <td>0.780277</td>\n",
       "      <td>0.260245</td>\n",
       "      <td>230.696598</td>\n",
       "      <td>0.026044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.973432</td>\n",
       "      <td>0.986529</td>\n",
       "      <td>0.769471</td>\n",
       "      <td>0.257268</td>\n",
       "      <td>343.013381</td>\n",
       "      <td>0.026044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">deduct 2.9</th>\n",
       "      <th>3</th>\n",
       "      <td>1.095035</td>\n",
       "      <td>1.046377</td>\n",
       "      <td>0.828041</td>\n",
       "      <td>0.272714</td>\n",
       "      <td>42.898332</td>\n",
       "      <td>0.020572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.076076</td>\n",
       "      <td>1.037272</td>\n",
       "      <td>0.816653</td>\n",
       "      <td>0.270006</td>\n",
       "      <td>64.869814</td>\n",
       "      <td>0.020572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.040506</td>\n",
       "      <td>1.019972</td>\n",
       "      <td>0.798785</td>\n",
       "      <td>0.265291</td>\n",
       "      <td>118.233508</td>\n",
       "      <td>0.020572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.030204</td>\n",
       "      <td>1.014907</td>\n",
       "      <td>0.794056</td>\n",
       "      <td>0.263976</td>\n",
       "      <td>145.227962</td>\n",
       "      <td>0.020572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.016827</td>\n",
       "      <td>1.008292</td>\n",
       "      <td>0.788054</td>\n",
       "      <td>0.262368</td>\n",
       "      <td>186.755147</td>\n",
       "      <td>0.020572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.999288</td>\n",
       "      <td>0.999552</td>\n",
       "      <td>0.780277</td>\n",
       "      <td>0.260245</td>\n",
       "      <td>238.708256</td>\n",
       "      <td>0.020572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.973432</td>\n",
       "      <td>0.986529</td>\n",
       "      <td>0.769471</td>\n",
       "      <td>0.257268</td>\n",
       "      <td>345.255247</td>\n",
       "      <td>0.020572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">deduct 3</th>\n",
       "      <th>3</th>\n",
       "      <td>1.095035</td>\n",
       "      <td>1.046377</td>\n",
       "      <td>0.828041</td>\n",
       "      <td>0.272714</td>\n",
       "      <td>45.148736</td>\n",
       "      <td>0.027951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.076076</td>\n",
       "      <td>1.037272</td>\n",
       "      <td>0.816653</td>\n",
       "      <td>0.270006</td>\n",
       "      <td>69.623487</td>\n",
       "      <td>0.027951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.040506</td>\n",
       "      <td>1.019972</td>\n",
       "      <td>0.798785</td>\n",
       "      <td>0.265291</td>\n",
       "      <td>130.808586</td>\n",
       "      <td>0.027951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.030204</td>\n",
       "      <td>1.014907</td>\n",
       "      <td>0.794056</td>\n",
       "      <td>0.263976</td>\n",
       "      <td>157.400456</td>\n",
       "      <td>0.027951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.016827</td>\n",
       "      <td>1.008292</td>\n",
       "      <td>0.788054</td>\n",
       "      <td>0.262368</td>\n",
       "      <td>202.293946</td>\n",
       "      <td>0.027951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.999288</td>\n",
       "      <td>0.999552</td>\n",
       "      <td>0.780277</td>\n",
       "      <td>0.260245</td>\n",
       "      <td>332.755546</td>\n",
       "      <td>0.027951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.973432</td>\n",
       "      <td>0.986529</td>\n",
       "      <td>0.769471</td>\n",
       "      <td>0.257268</td>\n",
       "      <td>404.980762</td>\n",
       "      <td>0.027951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">remap 12-&gt;-1, 345-&gt;1</th>\n",
       "      <th>3</th>\n",
       "      <td>1.250051</td>\n",
       "      <td>1.118006</td>\n",
       "      <td>0.892622</td>\n",
       "      <td>0.292399</td>\n",
       "      <td>50.580497</td>\n",
       "      <td>0.027936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.230278</td>\n",
       "      <td>1.109123</td>\n",
       "      <td>0.882086</td>\n",
       "      <td>0.290549</td>\n",
       "      <td>78.037876</td>\n",
       "      <td>0.027936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.195631</td>\n",
       "      <td>1.093391</td>\n",
       "      <td>0.865832</td>\n",
       "      <td>0.287205</td>\n",
       "      <td>145.140445</td>\n",
       "      <td>0.027936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.185318</td>\n",
       "      <td>1.088665</td>\n",
       "      <td>0.861413</td>\n",
       "      <td>0.286186</td>\n",
       "      <td>166.803648</td>\n",
       "      <td>0.027936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.172074</td>\n",
       "      <td>1.082562</td>\n",
       "      <td>0.855863</td>\n",
       "      <td>0.284895</td>\n",
       "      <td>196.226875</td>\n",
       "      <td>0.027936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.153871</td>\n",
       "      <td>1.074118</td>\n",
       "      <td>0.848526</td>\n",
       "      <td>0.283143</td>\n",
       "      <td>244.681028</td>\n",
       "      <td>0.027936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.127791</td>\n",
       "      <td>1.061904</td>\n",
       "      <td>0.838397</td>\n",
       "      <td>0.280638</td>\n",
       "      <td>353.417667</td>\n",
       "      <td>0.027936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       mse      rmse       mae      mape  \\\n",
       "metric_name          n_neighbors                                           \n",
       "adjusted cosine      3            1.113208  1.055023  0.848436  0.265783   \n",
       "                     5            1.092028  1.044940  0.835864  0.261168   \n",
       "                     10           1.041937  1.020688  0.810304  0.252746   \n",
       "                     12           1.024822  1.012270  0.802031  0.250146   \n",
       "                     15           1.002224  1.001044  0.791263  0.246783   \n",
       "                     20           0.969780  0.984701  0.776419  0.242251   \n",
       "                     30           0.921901  0.960081  0.754724  0.235772   \n",
       "cosine               3            1.095035  1.046377  0.828041  0.272714   \n",
       "                     5            1.076076  1.037272  0.816653  0.270006   \n",
       "                     10           1.040506  1.019972  0.798785  0.265291   \n",
       "                     12           1.030204  1.014907  0.794056  0.263976   \n",
       "                     15           1.016827  1.008292  0.788054  0.262368   \n",
       "                     20           0.999288  0.999552  0.780277  0.260245   \n",
       "                     30           0.973432  0.986529  0.769471  0.257268   \n",
       "deduct 2.5           3            1.095035  1.046377  0.828041  0.272714   \n",
       "                     5            1.076076  1.037272  0.816653  0.270006   \n",
       "                     10           1.040506  1.019972  0.798785  0.265291   \n",
       "                     12           1.030204  1.014907  0.794056  0.263976   \n",
       "                     15           1.016827  1.008292  0.788054  0.262368   \n",
       "                     20           0.999288  0.999552  0.780277  0.260245   \n",
       "                     30           0.973432  0.986529  0.769471  0.257268   \n",
       "deduct 2.75          3            1.095035  1.046377  0.828041  0.272714   \n",
       "                     5            1.076076  1.037272  0.816653  0.270006   \n",
       "                     10           1.040506  1.019972  0.798785  0.265291   \n",
       "                     12           1.030204  1.014907  0.794056  0.263976   \n",
       "                     15           1.016827  1.008292  0.788054  0.262368   \n",
       "                     20           0.999288  0.999552  0.780277  0.260245   \n",
       "                     30           0.973432  0.986529  0.769471  0.257268   \n",
       "deduct 2.9           3            1.095035  1.046377  0.828041  0.272714   \n",
       "                     5            1.076076  1.037272  0.816653  0.270006   \n",
       "                     10           1.040506  1.019972  0.798785  0.265291   \n",
       "                     12           1.030204  1.014907  0.794056  0.263976   \n",
       "                     15           1.016827  1.008292  0.788054  0.262368   \n",
       "                     20           0.999288  0.999552  0.780277  0.260245   \n",
       "                     30           0.973432  0.986529  0.769471  0.257268   \n",
       "deduct 3             3            1.095035  1.046377  0.828041  0.272714   \n",
       "                     5            1.076076  1.037272  0.816653  0.270006   \n",
       "                     10           1.040506  1.019972  0.798785  0.265291   \n",
       "                     12           1.030204  1.014907  0.794056  0.263976   \n",
       "                     15           1.016827  1.008292  0.788054  0.262368   \n",
       "                     20           0.999288  0.999552  0.780277  0.260245   \n",
       "                     30           0.973432  0.986529  0.769471  0.257268   \n",
       "remap 12->-1, 345->1 3            1.250051  1.118006  0.892622  0.292399   \n",
       "                     5            1.230278  1.109123  0.882086  0.290549   \n",
       "                     10           1.195631  1.093391  0.865832  0.287205   \n",
       "                     12           1.185318  1.088665  0.861413  0.286186   \n",
       "                     15           1.172074  1.082562  0.855863  0.284895   \n",
       "                     20           1.153871  1.074118  0.848526  0.283143   \n",
       "                     30           1.127791  1.061904  0.838397  0.280638   \n",
       "\n",
       "                                  predict_time  fit_time  \n",
       "metric_name          n_neighbors                          \n",
       "adjusted cosine      3               50.160261  0.028253  \n",
       "                     5               73.615602  0.028253  \n",
       "                     10             136.973672  0.028253  \n",
       "                     12             167.926390  0.028253  \n",
       "                     15             218.771822  0.028253  \n",
       "                     20             278.644197  0.028253  \n",
       "                     30             401.716500  0.028253  \n",
       "cosine               3               45.686521  0.021305  \n",
       "                     5               69.210290  0.021305  \n",
       "                     10             123.839174  0.021305  \n",
       "                     12             147.952476  0.021305  \n",
       "                     15             190.248673  0.021305  \n",
       "                     20             254.897141  0.021305  \n",
       "                     30             373.673951  0.021305  \n",
       "deduct 2.5           3               41.657907  0.021487  \n",
       "                     5             1272.386797  0.021487  \n",
       "                     10             126.539474  0.021487  \n",
       "                     12             174.205192  0.021487  \n",
       "                     15             214.388090  0.021487  \n",
       "                     20             270.670647  0.021487  \n",
       "                     30             326.099410  0.021487  \n",
       "deduct 2.75          3               41.850846  0.026044  \n",
       "                     5               64.360751  0.026044  \n",
       "                     10             115.235674  0.026044  \n",
       "                     12             135.968183  0.026044  \n",
       "                     15             169.061890  0.026044  \n",
       "                     20             230.696598  0.026044  \n",
       "                     30             343.013381  0.026044  \n",
       "deduct 2.9           3               42.898332  0.020572  \n",
       "                     5               64.869814  0.020572  \n",
       "                     10             118.233508  0.020572  \n",
       "                     12             145.227962  0.020572  \n",
       "                     15             186.755147  0.020572  \n",
       "                     20             238.708256  0.020572  \n",
       "                     30             345.255247  0.020572  \n",
       "deduct 3             3               45.148736  0.027951  \n",
       "                     5               69.623487  0.027951  \n",
       "                     10             130.808586  0.027951  \n",
       "                     12             157.400456  0.027951  \n",
       "                     15             202.293946  0.027951  \n",
       "                     20             332.755546  0.027951  \n",
       "                     30             404.980762  0.027951  \n",
       "remap 12->-1, 345->1 3               50.580497  0.027936  \n",
       "                     5               78.037876  0.027936  \n",
       "                     10             145.140445  0.027936  \n",
       "                     12             166.803648  0.027936  \n",
       "                     15             196.226875  0.027936  \n",
       "                     20             244.681028  0.027936  \n",
       "                     30             353.417667  0.027936  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(test_pkl_dict)\n",
    "results_df\\\n",
    "    .groupby([\"metric_name\", \"n_neighbors\"])\\\n",
    "    .agg(\n",
    "        {\n",
    "            \"mse\": \"mean\",\n",
    "            \"rmse\": \"mean\",\n",
    "            \"mae\": \"mean\",\n",
    "            \"mape\": \"mean\",\n",
    "            \"predict_time\": \"sum\",\n",
    "            \"fit_time\": \"mean\",\n",
    "        }\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
