{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6431eef8-ed8d-4cf7-89f9-0917a29350bf",
   "metadata": {},
   "source": [
    "# Collaborative Filtering from Scratch Abbreviated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26415e0c-fda6-45d4-bfab-091ef1fb486b",
   "metadata": {},
   "source": [
    "This notebook is used to demonstrate how to code and evaluate CF (Collaborative Filtering) using `sklearn`, `pandas`, `scipy`.\n",
    "\n",
    "It is called \"from scratch\" as it does not use the `surprise` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a50f7b6-489f-4df7-9fac-6ab8f0f96614",
   "metadata": {},
   "source": [
    "## 1. Load libraries and data, inspect data, convert data as sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b49b870-f06e-46a5-97c0-fc40544d07ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac595c76-bcd6-4d9d-8d26-eed3639ccc76",
   "metadata": {},
   "source": [
    "Specify your `ratings.csv`'s directory. The csv has 10k books, 50k users, 6M books, with ratings given between 1-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "076c9f51-346d-4113-a788-ca0eebca1f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5976479 entries, 0 to 5976478\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Dtype\n",
      "---  ------   -----\n",
      " 0   user_id  int64\n",
      " 1   book_id  int64\n",
      " 2   rating   int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 136.8 MB\n",
      "CPU times: user 572 ms, sys: 63.8 ms, total: 635 ms\n",
      "Wall time: 721 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "csv_dir = \"/home/zebalgebra/School/DVA/The-Last-Book-Bender/Data/Raw/\" # edit this line\n",
    "ratings_all_df = pd.read_csv(\n",
    "    os.path.join(csv_dir, \"ratings.csv\")\n",
    ")\n",
    "ratings_all_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0b724cd-2159-416d-93b1-233d194c11b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 232 ms, sys: 44.7 ms, total: 276 ms\n",
      "Wall time: 281 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ratings_all_csc = sp.sparse.csc_matrix(\n",
    "    (\n",
    "        np.array(ratings_all_df[\"rating\"]),\n",
    "        (\n",
    "            np.array(ratings_all_df[\"book_id\"]),\n",
    "            np.array(ratings_all_df[\"user_id\"])\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab9565c-615e-4364-8880-7239f5a4bd58",
   "metadata": {},
   "source": [
    "You can save this as an `npz` object using `save_npz`, and load it using `load_npz` (which is extremely fast)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3ba821-91ea-4e52-8a0e-e438dfd61f7e",
   "metadata": {},
   "source": [
    "## 2. Doing train test split for CV (on users)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ebbf5f-7373-4cfe-9b3d-bf6ca2d239b9",
   "metadata": {},
   "source": [
    "The following code block shows how to set up a 10-fold CV iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "528a8381-fdd0-4739-ae6d-c06b05a912c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting for fold 0 with train size 48082, test size 5343\n",
      "Splitting for fold 1 with train size 48082, test size 5343\n",
      "Splitting for fold 2 with train size 48082, test size 5343\n",
      "Splitting for fold 3 with train size 48082, test size 5343\n",
      "Splitting for fold 4 with train size 48082, test size 5343\n",
      "Splitting for fold 5 with train size 48083, test size 5342\n",
      "Splitting for fold 6 with train size 48083, test size 5342\n",
      "Splitting for fold 7 with train size 48083, test size 5342\n",
      "Splitting for fold 8 with train size 48083, test size 5342\n",
      "Splitting for fold 9 with train size 48083, test size 5342\n",
      "CPU times: user 108 ms, sys: 78.6 ms, total: 186 ms\n",
      "Wall time: 187 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=6242) # change this for other CV options\n",
    "user_ids = np.arange(53424 + 1) # the column range of our sparse matrix is 0-53424\n",
    "for fold, (user_ids_train, user_ids_test) in enumerate(kf.split(user_ids)):\n",
    "    print(f\"Splitting for fold {fold} with train size {len(user_ids_train)}, test size {len(user_ids_test)}\")\n",
    "    true_size = (10000 + 1, 53424 + 1)\n",
    "    ratings_train_csc = ratings_all_csc[:, user_ids_train]\n",
    "    ratings_test_csc = ratings_all_csc[:, user_ids_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989c7be2-c363-4af6-9004-e3013a957ea8",
   "metadata": {},
   "source": [
    "## 3. Utilities for Shifting Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471f397c-8a42-46fc-b173-6d810292f43f",
   "metadata": {},
   "source": [
    "The following code block defines utility functions for modifying underlying data of a csc matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27b854b7-15e8-49a5-8ba6-d896ba337b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilate(X: sp.sparse.csc_matrix, amount: float) -> None:\n",
    "    \"\"\"\n",
    "    deduct each nonempty entry of X by amount\n",
    "    (this mutates the underlying csc matrix)\n",
    "    for example, if take amount as baseline,\n",
    "    this function is useful.\n",
    "    \"\"\"\n",
    "    X.data = X.data - np.float64(amount)\n",
    "\n",
    "def value_remap(X: sp.sparse.csc_matrix, remapper: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    remap nonnull values of X based on remapper\n",
    "    (if remapper[i]=j, value of i is remapped to j)\n",
    "    note this function takes an np array (not an ordinary list)\n",
    "    \"\"\"\n",
    "    X.data = remapper[X.data.astype(int)].astype(np.float64)\n",
    "\n",
    "def make_mean_0(X: sp.sparse.csc_matrix) -> None:\n",
    "    \"\"\"\n",
    "    make columns of a csc_matrix have zero mean\n",
    "    (this function mutates the underlying data)\n",
    "    \"\"\"\n",
    "    X.data -= get_true_mean(X)\n",
    "\n",
    "def get_true_mean(X: sp.sparse.csc_matrix) -> None:\n",
    "    \"\"\"\n",
    "    compute mean of each column (over nonzero indices!)\n",
    "    this won't mutate the underlying X\n",
    "    a general helper function\n",
    "    \"\"\"\n",
    "    indexer = X.tocoo().col\n",
    "    v = np.array(X.sum(axis=0)).flatten()\n",
    "    c = np.array(X.minimum(1).sum(axis=0)).flatten()\n",
    "    return v[indexer] / np.maximum(c[indexer], 0.5)\n",
    "\n",
    "def test_mean_is_0(X: sp.sparse.csc_matrix, tol=10 ** (-10)) -> np.bool_:\n",
    "    \"\"\"\n",
    "    test if mean is 0 for each column\n",
    "    should be true on outputs of make_mean_0\n",
    "    \"\"\"\n",
    "    return np.abs(np.array(X.sum(axis=0))).max() < tol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eca11d5b-c416-4b33-bd2e-50d641ab4129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.    4.    5.    4.    3.    3.    4.    4.    4.    5.    4.    2.\n",
      "   2.    3.    4.  ]\n",
      " [ 1.    0.5   1.    0.5   0.25  0.25  0.5   0.5   0.5   1.    0.5  -0.5\n",
      "  -0.5   0.25  0.5 ]]\n",
      "CPU times: user 30.6 ms, sys: 43.7 ms, total: 74.3 ms\n",
      "Wall time: 74.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ratings_train_csc_demo = ratings_train_csc.copy().astype(np.float64)\n",
    "value_remap(ratings_train_csc_demo, np.array([0, -1, -0.5, 0.25, 0.5, 1]))\n",
    "# dilate(ratings_train_csc_demo, 2.5)\n",
    "# make_mean_0(ratings_train_csc_demo)\n",
    "# you can test if make_mean_0 is correct by uncommenting the next line\n",
    "# test_mean_is_0(ratings_train_csc_demo)\n",
    "print(np.vstack([ratings_train_csc.data, ratings_train_csc_demo.data])[:, :15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9c583c-0d4e-4191-9ed3-9d0b83169f0d",
   "metadata": {},
   "source": [
    "## 4. Getting Top Similarity Scores and Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8bf096-61d3-4f66-87cf-4d931cb0f3a1",
   "metadata": {},
   "source": [
    "With these functions, we can plug these into `NearestNeighbors` to get top recommendations based on our desired metrics. Take adjusted cosine as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e925ee1-058a-4424-8224-90d9927a5059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.9 s, sys: 1.12 s, total: 12 s\n",
      "Wall time: 12.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((5342, 5), (5342, 5))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# preprocess train and test sets\n",
    "ratings_train_csc_modified = ratings_train_csc.copy().astype(np.float64)\n",
    "ratings_test_csc_modified = ratings_test_csc.copy().astype(np.float64)\n",
    "# value_remap(ratings_train_csc_modified, np.array([0, -1, -0.5, 0.25, 0.5, 1]))\n",
    "# value_remap(ratings_test_csc_modified, np.array([0, -1, -0.5, 0.25, 0.5, 1]))\n",
    "make_mean_0(ratings_train_csc_modified)\n",
    "make_mean_0(ratings_test_csc_modified)\n",
    "# fit knn\n",
    "n_neighbors = 5\n",
    "knn = NearestNeighbors(metric=\"cosine\")\n",
    "knn.fit(ratings_train_csc_modified.transpose()) # sklearn's convention is to use rows\n",
    "neigh_dist, neigh_ind = knn.kneighbors(\n",
    "    X=ratings_test_csc_modified.transpose(),\n",
    "    n_neighbors=n_neighbors\n",
    ") # this step took ~12 seconds on my machine\n",
    "sim_scores = 1 - neigh_dist\n",
    "sim_scores.shape, neigh_ind.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455c8c77-a655-4f26-af45-a3ea4d34dcfd",
   "metadata": {},
   "source": [
    "We abstract this procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f2c89f5-9f42-496a-9139-e008f14e05f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_neigh_dist_ind(\n",
    "    ratings_train_csc_modified,\n",
    "    ratings_test_csc_modified,\n",
    "    n_neighbors=5\n",
    "):\n",
    "    knn = NearestNeighbors(metric=\"cosine\")\n",
    "    knn.fit(ratings_train_csc_modified.transpose())\n",
    "    neigh_dist, neigh_ind = knn.kneighbors(\n",
    "        X=ratings_test_csc_modified.transpose(),\n",
    "        n_neighbors=n_neighbors\n",
    "    )\n",
    "    return 1 - neigh_dist, neigh_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6239e8-6996-47dc-b2a8-ca6c6c91944c",
   "metadata": {},
   "source": [
    "## 5. Predict Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3aa276-c729-47be-a87e-076d18b8df07",
   "metadata": {},
   "source": [
    "Now we have rankings and similarity scores. To evaluate model performance - such as MAP, MSE - we need to predict ratings on test user's ratings. We only evaluate on those with ground truths.\n",
    "\n",
    "Fix a number $k$ for desired neighborhood size. We use the approximated metric (here $u$ is a user, $i$ is an item):\n",
    "\n",
    "$$\\hat{r}_{ui}=\\bar{r}_u+\\frac{\\sum_{v\\in N_u}\\delta_{i, I_v}\\text{sim}_{uv}(r_{vi}-\\bar{r}_v)}{\\sum_{v\\in N_u}\\delta_{i, I_v}|\\text{sim}_{uv}|},\\quad\\text{for test user }u,\\text{ an item }i\\text{ that }u\\text{ has rated}$$\n",
    "\n",
    "where $N_{u}$ is the set of top $k$ neighbors of $u$, $\\text{sim}_{uv}$ is similarity score between $u,v$, $r_{vi}$ is true rating. Although this metric is a suboptimal one, this allows more vectorization to our computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d3a742d-449d-41a2-ba77-a6d4f09316c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 108 ms, sys: 20.4 ms, total: 128 ms\n",
      "Wall time: 128 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# compute vector of means\n",
    "ratings_train_csc_mean = get_true_mean(ratings_train_csc)\n",
    "ratings_test_csc_mean = get_true_mean(ratings_test_csc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebca117e-598b-480c-91bc-f21895c13c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.09 s, sys: 33.1 ms, total: 3.13 s\n",
      "Wall time: 3.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get number of total items, test users\n",
    "n_items = ratings_test_csc.shape[0]\n",
    "n_users_test = len(user_ids_test)\n",
    "\n",
    "# initialize arrays\n",
    "ratings_test_csc_predicted = ratings_test_csc.copy()\n",
    "ratings_test_csc_predicted.data = np.zeros(len(ratings_test_csc.data))\n",
    "\n",
    "# compute vector of means\n",
    "ratings_train_mean = get_true_mean(ratings_train_csc)\n",
    "ratings_test_mean = get_true_mean(ratings_test_csc)\n",
    "\n",
    "# these two sprase arrays will represent the numerator, denominator involving the delta terms\n",
    "numer_test_csc_ratings = ratings_test_csc_predicted.copy()\n",
    "denom_test_csc_ratings = ratings_test_csc_predicted.copy()\n",
    "\n",
    "# prediction loop\n",
    "start_ind = 0\n",
    "for i in np.arange(n_users_test):\n",
    "    denom_col = np.zeros(n_items)\n",
    "    numer_col = np.zeros(n_items)\n",
    "    baseline_items = ratings_test_csc.indices[\n",
    "        ratings_test_csc.indptr[i]: ratings_test_csc.indptr[i + 1]\n",
    "    ]\n",
    "    n_baseline_items = len(baseline_items)\n",
    "    end_ind = start_ind + n_baseline_items\n",
    "    for j, sim_score in zip(neigh_ind[i], sim_scores[i]):\n",
    "        r_v = ratings_train_csc[:, j].toarray().flatten()[baseline_items]\n",
    "        mu_v = ratings_train_mean[j]\n",
    "        w = sim_score * np.minimum(1, r_v)\n",
    "        numer_test_csc_ratings.data[start_ind: end_ind] += w * (r_v - mu_v)\n",
    "        denom_test_csc_ratings.data[start_ind: end_ind] += w\n",
    "    start_ind = end_ind\n",
    "ratings_test_csc_predicted.data =\\\n",
    "    ratings_test_mean[ratings_test_csc_predicted.tocoo().col] +\\\n",
    "    numer_test_csc_ratings.data /\\\n",
    "    np.maximum(\n",
    "        denom_test_csc_ratings.data,\n",
    "        10 ** (-30)\n",
    "    )\n",
    "ratings_diffs = ratings_test_csc.data - ratings_test_csc_predicted.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6955d7a6-6bd2-484f-9af6-a1fdb34166e4",
   "metadata": {},
   "source": [
    "For future reference, we abstract the rating prediction part as a function. Since we will test different neighborhood sizes, to avoid repeated calculation, some modifications are made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "927ff21b-96c9-48d2-9e70-251807b066d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_numer_denom(\n",
    "    numer_test_csc_ratings,\n",
    "    denom_test_csc_ratings,\n",
    "    ratings_test_csc,\n",
    "    ratings_train_csc,\n",
    "    ratings_train_mean,\n",
    "    n_items,\n",
    "    n_users_test,\n",
    "    pos # position of which neighbor to update\n",
    "):\n",
    "    \"\"\"\n",
    "    this function mutates the input csc matrices\n",
    "    \"\"\"\n",
    "    start_ind = 0\n",
    "    for i in np.arange(n_users_test):\n",
    "        denom_col = np.zeros(n_items)\n",
    "        numer_col = np.zeros(n_items)\n",
    "        baseline_items = ratings_test_csc.indices[\n",
    "            ratings_test_csc.indptr[i]: ratings_test_csc.indptr[i + 1]\n",
    "        ]\n",
    "        n_baseline_items = len(baseline_items)\n",
    "        end_ind = start_ind + n_baseline_items\n",
    "        j = neigh_ind[i, pos]\n",
    "        sim_score = sim_scores[i, pos]\n",
    "        r_v = ratings_train_csc[:, j]\\\n",
    "                .toarray().flatten()[baseline_items]\n",
    "        mu_v = ratings_train_mean[j]\n",
    "        w = sim_score * np.minimum(1, r_v)\n",
    "        numer_test_csc_ratings.data[start_ind: end_ind] += w * (r_v - mu_v)\n",
    "        denom_test_csc_ratings.data[start_ind: end_ind] += w\n",
    "        start_ind = end_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ca18be3-8526-44d9-8755-a894000758cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_prediction(\n",
    "    ratings_test_csc_predicted,\n",
    "    ratings_test_mean,\n",
    "    numer_test_csc_ratings,\n",
    "    denom_test_csc_ratings\n",
    "):\n",
    "    ratings_test_csc_predicted.data =\\\n",
    "        ratings_test_mean[ratings_test_csc_predicted.tocoo().col] +\\\n",
    "        numer_test_csc_ratings.data /\\\n",
    "        np.maximum(\n",
    "            denom_test_csc_ratings.data,\n",
    "            10 ** (-30)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269fd1df-9d89-4c1c-a894-be4221f981c3",
   "metadata": {},
   "source": [
    "## 3.4 Collaborative Filtering - Evaluating ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84c3354-b7be-416b-8cc2-1ed4fea1d3e2",
   "metadata": {},
   "source": [
    "Let us evaluate our model using just RMSE, MAE - in an offline setting and with only rating values between 1, 2, 3, 4, 5, RMSE and MAE are the most straightforward ones.\n",
    "\n",
    "The formulas:\n",
    "\n",
    "$$\\text{RMSE}=\\sqrt{|T|^{-1}\\sum_{(u, i)\\in T}(r_{ui}-\\hat{r}_{ui})^2}$$\n",
    "\n",
    "$$\\text{MAE}={|T|^{-1}\\sum_{(u, i)\\in T}|r_{ui}-\\hat{r}_{ui}|}$$\n",
    "\n",
    "More succinctly, RMSE, MAE corresponds to the 2 and 1 - norm of the difference between predictions and ratings. We can compute them using `numpy.linalg.norm`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cde8bc97-ccf1-4a25-9088-8abcfeae7923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (root mean square error): 1.0384170135083597\n",
      "MAE (mean absolute error): 0.8331371945569175\n"
     ]
    }
   ],
   "source": [
    "def eval_error(ratings_diffs: np.ndarray, sense:str=\"RMSE\"):\n",
    "    \"\"\"\n",
    "    function for evaluating RMSE, MAE of a ratings_diffs array\n",
    "    sense can be \"RMSE\" or \"MAE\", no other options for now\n",
    "    \"\"\"\n",
    "    if sense not in {\"RMSE\", \"MAE\"}:\n",
    "        raise NotImplementedError\n",
    "    p = {\"RMSE\": 2, \"MAE\": 1}[sense]\n",
    "    return np.linalg.norm(ratings_diffs, p) / ratings_diffs.shape[0] ** (1 / p)\n",
    "print(f'RMSE (root mean square error): {eval_error(ratings_diffs, \"RMSE\")}')\n",
    "print(f'MAE (mean absolute error): {eval_error(ratings_diffs, \"MAE\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf54fab4-7cd8-4165-8ccf-bc115ce92484",
   "metadata": {},
   "source": [
    "We can probably do better than this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804a48fe-8c98-448b-b228-0781edb86502",
   "metadata": {},
   "source": [
    "## 3.5 Collaborative Filtering - Putting all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7828a77b-fdc1-4c1b-a347-911f10354324",
   "metadata": {},
   "source": [
    "Let us make clear on which models that we want to try:\n",
    "1. Number of folds for CV - we fix this to be 10 for our task.\n",
    "2. Metrics to use - we use shifting by 0 (ordinary cosine), 2.5, 2.75, 2.9, 3, adjusted cosine, and the mapping of 1-2 to -1, 3-5 to 1.\n",
    "3. Number of neighborhoods to use - we use 3, 5, 10, 12, 15, 20, 30 (note that the time neede to generate predictions is roughly linear to number of neighborhoods used)\n",
    "\n",
    "The evaluation pipeline:\n",
    "* (A) Load data and convert to sparse matrix\n",
    "* (B) For each train test split index for a 10 fold CV:\n",
    "    * (B1) Build train test set (by sparse array column slicing), Calculate some metadata\n",
    "    * (B2) For every metrics that we want to use:\n",
    "        * (B2-1) Copy train test set and modify its values\n",
    "        * (B2-2) Build knn model from train set, and fit this model to test set to grab 200 nearest neighbors (will time this step) and simimilarity scores.\n",
    "        * (B2-3) For for each number of neighborhoods we want to try:\n",
    "            * (B2-3-1) Update the numerator, denominator term, update predicted scores for each user-item in train set (will time this step).\n",
    "            * (B2-3-2) Get difference of ratings (predicted versus baseline in train set)\n",
    "            * (B2-3-3) Calculate MAE and RMSE.\n",
    "            * (B2-3-4) Save MAE, RMSE, prediction generation time.\n",
    "* (C) Combine (MAE, RMSE, prediction generation time) for each pair of (metric, number of neighborhood) across the 10 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad6fcaee-f52f-4a12-8ce6-df4625a549a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold=0 started.\n",
      " - fold=0, metric='cosine' started.\n",
      " - fold=0, metric='deduct 2.5' started.\n",
      " - fold=0, metric='deduct 2.75' started.\n",
      " - fold=0, metric='deduct 2.9' started.\n",
      " - fold=0, metric='deduct 3' started.\n",
      " - fold=0, metric='adjusted cosine' started.\n",
      " - fold=0, metric='remap 12->-1, 345->1' started.\n",
      " - fold=0, metric='remap 1,5->-+1, 24->-+0.5, 3->0.25' started.\n",
      "fold=0 ended. used 1316.2100937366486 seconds.\n",
      "fold=1 started.\n",
      " - fold=1, metric='cosine' started.\n",
      " - fold=1, metric='deduct 2.5' started.\n",
      " - fold=1, metric='deduct 2.75' started.\n",
      " - fold=1, metric='deduct 2.9' started.\n",
      " - fold=1, metric='deduct 3' started.\n",
      " - fold=1, metric='adjusted cosine' started.\n",
      " - fold=1, metric='remap 12->-1, 345->1' started.\n",
      " - fold=1, metric='remap 1,5->-+1, 24->-+0.5, 3->0.25' started.\n",
      "fold=1 ended. used 1329.7925810813904 seconds.\n",
      "fold=2 started.\n",
      " - fold=2, metric='cosine' started.\n",
      " - fold=2, metric='deduct 2.5' started.\n",
      " - fold=2, metric='deduct 2.75' started.\n",
      " - fold=2, metric='deduct 2.9' started.\n",
      " - fold=2, metric='deduct 3' started.\n",
      " - fold=2, metric='adjusted cosine' started.\n",
      " - fold=2, metric='remap 12->-1, 345->1' started.\n",
      " - fold=2, metric='remap 1,5->-+1, 24->-+0.5, 3->0.25' started.\n",
      "fold=2 ended. used 1250.8452475070953 seconds.\n",
      "fold=3 started.\n",
      " - fold=3, metric='cosine' started.\n",
      " - fold=3, metric='deduct 2.5' started.\n",
      " - fold=3, metric='deduct 2.75' started.\n",
      " - fold=3, metric='deduct 2.9' started.\n",
      " - fold=3, metric='deduct 3' started.\n",
      " - fold=3, metric='adjusted cosine' started.\n",
      " - fold=3, metric='remap 12->-1, 345->1' started.\n",
      " - fold=3, metric='remap 1,5->-+1, 24->-+0.5, 3->0.25' started.\n",
      "fold=3 ended. used 1319.3828384876251 seconds.\n",
      "fold=4 started.\n",
      " - fold=4, metric='cosine' started.\n",
      " - fold=4, metric='deduct 2.5' started.\n",
      " - fold=4, metric='deduct 2.75' started.\n",
      " - fold=4, metric='deduct 2.9' started.\n",
      " - fold=4, metric='deduct 3' started.\n",
      " - fold=4, metric='adjusted cosine' started.\n",
      " - fold=4, metric='remap 12->-1, 345->1' started.\n",
      " - fold=4, metric='remap 1,5->-+1, 24->-+0.5, 3->0.25' started.\n",
      "fold=4 ended. used 1338.1224949359894 seconds.\n",
      "fold=5 started.\n",
      " - fold=5, metric='cosine' started.\n",
      " - fold=5, metric='deduct 2.5' started.\n",
      " - fold=5, metric='deduct 2.75' started.\n",
      " - fold=5, metric='deduct 2.9' started.\n",
      " - fold=5, metric='deduct 3' started.\n",
      " - fold=5, metric='adjusted cosine' started.\n",
      " - fold=5, metric='remap 12->-1, 345->1' started.\n",
      " - fold=5, metric='remap 1,5->-+1, 24->-+0.5, 3->0.25' started.\n",
      "fold=5 ended. used 1332.884400844574 seconds.\n",
      "fold=6 started.\n",
      " - fold=6, metric='cosine' started.\n",
      " - fold=6, metric='deduct 2.5' started.\n",
      " - fold=6, metric='deduct 2.75' started.\n",
      " - fold=6, metric='deduct 2.9' started.\n",
      " - fold=6, metric='deduct 3' started.\n",
      " - fold=6, metric='adjusted cosine' started.\n",
      " - fold=6, metric='remap 12->-1, 345->1' started.\n",
      " - fold=6, metric='remap 1,5->-+1, 24->-+0.5, 3->0.25' started.\n",
      "fold=6 ended. used 1659.3751316070557 seconds.\n",
      "fold=7 started.\n",
      " - fold=7, metric='cosine' started.\n",
      " - fold=7, metric='deduct 2.5' started.\n",
      " - fold=7, metric='deduct 2.75' started.\n",
      " - fold=7, metric='deduct 2.9' started.\n",
      " - fold=7, metric='deduct 3' started.\n",
      " - fold=7, metric='adjusted cosine' started.\n",
      " - fold=7, metric='remap 12->-1, 345->1' started.\n",
      " - fold=7, metric='remap 1,5->-+1, 24->-+0.5, 3->0.25' started.\n",
      "fold=7 ended. used 1330.0506510734558 seconds.\n",
      "fold=8 started.\n",
      " - fold=8, metric='cosine' started.\n",
      " - fold=8, metric='deduct 2.5' started.\n",
      " - fold=8, metric='deduct 2.75' started.\n",
      " - fold=8, metric='deduct 2.9' started.\n",
      " - fold=8, metric='deduct 3' started.\n",
      " - fold=8, metric='adjusted cosine' started.\n",
      " - fold=8, metric='remap 12->-1, 345->1' started.\n",
      " - fold=8, metric='remap 1,5->-+1, 24->-+0.5, 3->0.25' started.\n",
      "fold=8 ended. used 1458.5041472911835 seconds.\n",
      "fold=9 started.\n",
      " - fold=9, metric='cosine' started.\n",
      " - fold=9, metric='deduct 2.5' started.\n",
      " - fold=9, metric='deduct 2.75' started.\n",
      " - fold=9, metric='deduct 2.9' started.\n",
      " - fold=9, metric='deduct 3' started.\n",
      " - fold=9, metric='adjusted cosine' started.\n",
      " - fold=9, metric='remap 12->-1, 345->1' started.\n",
      " - fold=9, metric='remap 1,5->-+1, 24->-+0.5, 3->0.25' started.\n",
      "fold=9 ended. used 1572.8210508823395 seconds.\n",
      "CPU times: user 4h 36min 5s, sys: 4h 23min 40s, total: 8h 59min 46s\n",
      "Wall time: 3h 51min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = []\n",
    "# specify metadata\n",
    "metric_names = [\n",
    "                \"cosine\", *[f\"deduct {amount}\" for amount in [2.5, 2.75, 2.9, 3]],\n",
    "                \"adjusted cosine\",\n",
    "                \"remap 12->-1, 345->1\",\n",
    "                \"remap 1,5->-+1, 24->-+0.5, 3->0.25\"\n",
    "            ]\n",
    "metric_funcs = [\n",
    "                lambda x: dilate(x, 0),\n",
    "                lambda x: dilate(x, 2.5),\n",
    "                lambda x: dilate(x, 2.75),\n",
    "                lambda x: dilate(x, 2.9),\n",
    "                lambda x: dilate(x, 3),\n",
    "                make_mean_0,\n",
    "                lambda x: value_remap(x, np.array([0, -1, -1, 1, 1, 1])),\n",
    "                lambda x: value_remap(x, np.array([0, -1, -0.5, 0.25, 0.5, 1]))\n",
    "            ]\n",
    "n_neighbors = 200\n",
    "# (A)\n",
    "csv_dir = \"/home/zebalgebra/School/DVA/The-Last-Book-Bender/Data/Raw/\"\n",
    "ratings_all_df = pd.read_csv(\n",
    "    os.path.join(csv_dir, \"ratings.csv\")\n",
    ")\n",
    "ratings_all_csc = sp.sparse.csc_matrix(\n",
    "    (\n",
    "        ratings_all_df[\"rating\"],\n",
    "        (\n",
    "            ratings_all_df[\"book_id\"],\n",
    "            ratings_all_df[\"user_id\"]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "# (B)\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=6242)\n",
    "user_ids = np.arange(53424 + 1)\n",
    "for (fold, (user_ids_train, user_ids_test)) in enumerate(kf.split(user_ids)):\n",
    "    print(f\"fold={fold} started.\")\n",
    "    fold_time_start = time.time()\n",
    "    # (B1)\n",
    "    ratings_train_csc = ratings_all_csc[:, user_ids_train]\n",
    "    ratings_test_csc = ratings_all_csc[:, user_ids_test]\n",
    "    ratings_train_mean = get_true_mean(ratings_train_csc)\n",
    "    ratings_test_mean = get_true_mean(ratings_test_csc)\n",
    "    n_items = ratings_test_csc.shape[0]\n",
    "    n_users_test = len(user_ids_test)\n",
    "    # (B2)\n",
    "    for metric_name, metric_func in zip(metric_names, metric_funcs):\n",
    "        print(f\" - fold={fold}, metric='{metric_name}' started.\")\n",
    "        # (B2-1)\n",
    "        ratings_train_csc_modified = ratings_train_csc.copy().astype(np.float64)\n",
    "        ratings_test_csc_modified = ratings_test_csc.copy().astype(np.float64)\n",
    "        metric_func(ratings_train_csc_modified)\n",
    "        metric_func(ratings_test_csc_modified)\n",
    "        # (B2-2)\n",
    "        sim_scores, neigh_ind = get_top_neigh_dist_ind(\n",
    "            ratings_train_csc_modified,\n",
    "            ratings_test_csc_modified,\n",
    "            n_neighbors=n_neighbors\n",
    "        )\n",
    "        # (B2-3)\n",
    "        ratings_test_csc_predicted = ratings_test_csc.copy()\n",
    "        ratings_test_csc_predicted.data = np.zeros(len(ratings_test_csc.data))\n",
    "        numer_test_csc_ratings = ratings_test_csc_predicted.copy()\n",
    "        denom_test_csc_ratings = ratings_test_csc_predicted.copy()\n",
    "        for pos in range(n_neighbors):\n",
    "            update_numer_denom(\n",
    "                numer_test_csc_ratings,\n",
    "                denom_test_csc_ratings,\n",
    "                ratings_test_csc,\n",
    "                ratings_train_csc,\n",
    "                ratings_train_mean,\n",
    "                n_items,\n",
    "                n_users_test,\n",
    "                pos\n",
    "            )\n",
    "            update_prediction(\n",
    "                ratings_test_csc_predicted,\n",
    "                ratings_test_mean,\n",
    "                numer_test_csc_ratings,\n",
    "                denom_test_csc_ratings\n",
    "            )\n",
    "            ratings_diffs = ratings_test_csc_predicted.data - ratings_test_csc.data\n",
    "            # (B2-3-3)\n",
    "            rmse = eval_error(ratings_diffs, \"RMSE\")\n",
    "            mae = eval_error(ratings_diffs, \"MAE\")\n",
    "            # (B2-3-4)\n",
    "            results.append(\n",
    "                {\n",
    "                    \"fold\": fold,\n",
    "                    \"metric_name\": metric_name,\n",
    "                    \"n_neighbors\": pos + 1,\n",
    "                    \"rmse\": rmse,\n",
    "                    \"mae\": mae\n",
    "                }\n",
    "            )\n",
    "    print(f\"fold={fold} ended. used {time.time()-fold_time_start} seconds.\")\n",
    "    with open(f'checkpoint_fold_{fold}.pkl', 'wb') as f:\n",
    "        pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed919f1-4277-4576-967a-c1da1aea422b",
   "metadata": {},
   "source": [
    "The testing results are saved in `checkpoint_fold_0.pkl` - `checkpoint_fold_10.pkl`.\n",
    "\n",
    "The plot for RMSE and MAE are in `CF-CV-Results.ipynb`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
